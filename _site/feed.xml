<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Moody Rd</title>
    <description>A blog about machine learning.</description>
    <link>http://blog.mrtz.org/</link>
    <atom:link href="http://blog.mrtz.org/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 13 Mar 2015 14:21:16 -0700</pubDate>
    <lastBuildDate>Fri, 13 Mar 2015 14:21:16 -0700</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Towards practicing differential privacy</title>
        <description>&lt;p&gt;More than a year ago I wrote an article with the provocative title: &lt;a href=&quot;http://blog.mrtz.org/2013/08/21/dp-practical.html&quot;&gt;Is
Differential Privacy practical?&lt;/a&gt;
The post was essentially one big buildup for
an epic follow-up post that I simply never wrote. Since then dozens
have asked me for an answer to this urgent question. Recently, after the post
hit the front page of Hacker News, half a dozen emails
inquired about the follow-up post that I had promised. Some &lt;a href=&quot;https://news.ycombinator.com/item?id=9184479&quot;&gt;speculated&lt;/a&gt; that owing to &lt;a href=&quot;http://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines&quot;&gt;Betteridge’s law of
headlines&lt;/a&gt;, the
answer was simply &lt;em&gt;no&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Despite my venerable history of failing on various commitments and my apparent
peace with it, this situation went too far even by my own low standards.
So, I decided to write a not so epic version of that promised blog
post.&lt;/p&gt;

&lt;h2 id=&quot;the-california-public-utilities-commission&quot;&gt;The California Public Utilities Commission&lt;/h2&gt;

&lt;p&gt;I’ll arrange my thoughts around the case of the &lt;a href=&quot;http://www.cpuc.ca.gov/puc/&quot;&gt;California Public
Utilities Commission&lt;/a&gt; (CPUC). The CPUC is a
regulatory agency that regulates privately owned public utilities in
California. In recent years there has been political pressure on the utilities
to give third parties access to smart meter data. As discussed in &lt;a href=&quot;http://blog.mrtz.org/2013/08/21/dp-practical.html&quot;&gt;my previous
post&lt;/a&gt;, smart meter data is
of enormous value to many, but comes with serious privacy challenges.&lt;/p&gt;

&lt;p&gt;To settle these issues the CPUC organized a major legal proceeding with the
goal of creating rules that provide access to energy usage data to local
government entities, researchers, and state and federal agencies while
establishing procedures that protect the privacy of consumer data.&lt;/p&gt;

&lt;p&gt;I served as a privacy expert within the proceeding together with Cynthia Dwork, Lee Tien from the
&lt;a href=&quot;http://www.eff.org&quot;&gt;EFF&lt;/a&gt;, and Jennifer Urban and her team from Berkeley.
Our goal was to inform various parties about the pitfalls
of insufficient privacy mechanisms and to propose better ones. Our proposed
solution focused on differential privacy for the uses cases in which it made
sense. There were a number of use cases that the CPUC considered. Not all of
them were well suited for differential privacy to begin with.&lt;/p&gt;

&lt;h3 id=&quot;a-proposed-decision&quot;&gt;A proposed decision&lt;/h3&gt;

&lt;p&gt;My involvement with the case ended in 2014 after a &lt;a href=&quot;http://docs.cpuc.ca.gov/PublishedDocs/Efile/G000/M088/K947/88947979.PDF&quot;&gt;proposed
decision&lt;/a&gt;
of the administrative judge. To summarize a 120 page document in one sentence,
the ruling did not endorse differential privacy strongly enough for me to further pursue the case actively.
Nevertheless, there was still significant interest in differential privacy
from some of the utilities. I believe that one utilities company
engaged with Microsoft with the goal of building a prototype of a
differentially private solution for their data sharing needs.&lt;/p&gt;

&lt;p&gt;The ruling was disappointing from my perspective in that it did not advocate
the use of differential privacy in any of the use cases. Meanwhile it shot
down several uses cases essentially not giving the use
case sponsors meaningful access to energy data at all. In those cases
differential privacy could’ve provided an obviously better trade-off for everyone.&lt;/p&gt;

&lt;p&gt;The ruling didn’t so much reflect a technical verdict about differential privacy. Rather it reflected our inability to successfully anticipate and maneuver the highly complex political and legal environment in which the decision was made.&lt;/p&gt;

&lt;h3 id=&quot;a-post-mortem&quot;&gt;A post mortem&lt;/h3&gt;

&lt;p&gt;Our proposal based on differential privacy initially met with resoundingly positive
responses when we first presented it to the administrative judge and various
parties present in the meeting. We did however face bitter opposition from a
group of researchers who sponsored one use case. Those researchers, who had
been working with raw smart meter data in the past, were worried that differential privacy
would create an obstacle for them. We quickly realized that it would be
difficult to agree with them on the extent to which their research practices
are compatible with differential privacy. So, we specifically excluded their
use case from the scope of our proposal focusing on some of the remaining use cases instead. This
didn’t stop the researchers from lobbying relentlessly against differential
privacy. In particular, they filed a last minute comment in which they
attacked differential privacy sharply based on many profound factual misunderstandings
of the privacy notion. Due to the perfect timing of their comment, we were
unable to submit a rebuttal. In the end, I believe this alone was enough for the
administrative judge to conclude that the use of differential privacy was at
present too controversial to be proposed as a solution in the ruling.&lt;/p&gt;

&lt;p&gt;My point is not to criticize this group of researchers. I’m sympathetic with
them. They’ve been working with energy data for many years. They’re doing important work which is probably already difficult enough as it is. We respected their
position and did not want to interfere with their research. My guess
is that their research practices are actually largely consistent with what’s
possible under differential privacy, but that’s an entirely separate
discussion.&lt;/p&gt;

&lt;p&gt;What’s tragic is that their opposition ended up hurting a consumer advocacy
group who could’ve used differential privacy as a means to gain &lt;em&gt;more&lt;/em&gt; access to
energy data than they were able to get in the end (essentially nothing). There was a lot of miscommunication throughout the proceeding that clearly didn’t help. For instance, initially the consumer advocacy group proposed their own ad-hoc privacy solution (which we didn’t support). Only later did we find some common ground. In hindsight, we should’ve agreed on and jointly represented the same solution from the beginning. In my understanding, the use case didn’t require more than the kind of aggregate usage statistics that we could’ve easily produced while preserving differential privacy without any major engineering efforts.&lt;/p&gt;

&lt;h2 id=&quot;towards-practicing-differential-privacy&quot;&gt;Towards practicing differential privacy&lt;/h2&gt;

&lt;p&gt;Drawing on my experience with the CPUC case, I want to end with some concrete
suggestions and questions hoping that they will help others when applying
differential privacy. When I speak of “the community”, I will make some very broad generalizations knowing full well that in each instance there are certainly exceptions to what I claim. The discussion below is by no means a survey as it contains very few links to the rich literature on differential privacy. I strongly encourage you to fill in relevant missing pointers in the comments.&lt;/p&gt;

&lt;h3 id=&quot;focus-on-win-win-applications&quot;&gt;Focus on win-win applications&lt;/h3&gt;

&lt;p&gt;Apply differential privacy as a tool to provide access to data where
currently access is problematic due to privacy regulations. Don’t fight the
data analyst. Don’t play the moral police. Imagine you are the analyst.&lt;/p&gt;

&lt;p&gt;As a privacy expert,
you will find yourself having to shoot down inadequate solutions all the time.
Why can’t we just omit those 18 sensitive attributes like in the HIPAA safe harbor provision?
Why isn’t it safe to release any statistic that is aggregated over at least 15 households in which no
single household contributes 15% of the total number (i.e., the “15/15” rule)?
Such ad-hoc rules sound intuitively appealing to non-experts. Refuting them is time-consuming and makes
you look defensive.&lt;/p&gt;

&lt;p&gt;Rather than shooting down what doesn’t work, point out why differential privacy is better than those solutions not just from a privacy perspective but rather from a &lt;em&gt;utility&lt;/em&gt; perspective. Unlike these solutions,
differential privacy does not alter your data set at all. In particular, from a statistical perspective
you do not change the distribution from which the data were drawn. This is an incredibly powerful
proposition. I think that data analysis with differential privacy can be vastly more useful than
what you get after applying, for instance, the HIPAA safe harbor mechanism.&lt;/p&gt;

&lt;p&gt;My point is that there are many “win/win” applications of differential privacy where it simultaneously can give better utility and better privacy than its alternatives. As the CPUC case showed, sometimes the choice is even between no access at all and differentially private access. It’s really a no-brainer. We should start with such applications instead of arguing about completely unrestricted access versus differentially private access.&lt;/p&gt;

&lt;h3 id=&quot;dont-empower-the-naysayers&quot;&gt;Don’t empower the naysayers&lt;/h3&gt;

&lt;p&gt;In my opinion, for differential privacy to be a major success in practice it would be sufficient if it were  successful in &lt;em&gt;some&lt;/em&gt; applications but certainly not in &lt;em&gt;all&lt;/em&gt;—not even in most. There’s a culture of criticizing differential privacy based on the perfectly correct observation that some differentially private algorithm (say, Laplace) didn’t give enough utility in some application. These kind of observations—valid as they may be—say very little about the potential of differential privacy in practice. First of all, they only evaluate one algorithm while there could be much better algorithms. Second, they commit to one specific application and, more importantly, one particular modeling of the problem. Perhaps there’s a different approach to the same problem that’s more compatible with differential privacy. It’s simply impossible to rule out differential privacy as a solution through these kind of straw man experiments.&lt;/p&gt;

&lt;p&gt;The differential privacy community is partially at blame for empowering the naysayers, since they have advertised differential privacy as a &lt;em&gt;universal&lt;/em&gt; solution concept to the privacy problem. This is theoretically true in some sense, but the situation in practice is much more delicate. So, stop feeding the naysayers. Start presenting differential privacy as a promising technology for &lt;em&gt;some&lt;/em&gt; applications but certainly not &lt;em&gt;all&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;change-your-narrative&quot;&gt;Change your narrative&lt;/h3&gt;

&lt;p&gt;Don’t present differential privacy as a fear inducing crypto hammer
designed to obfuscate data access. That’s not what it is.
Differential privacy is a rigorous way of doing machine learning, not a way of
preventing machine learning from being done. We understand perfectly well now
that differential privacy is a stability guarantee which is fundamentally
aligned with the central goal of statistics, namely, to learn from data about the population
as a whole and not about specific individuals. This understanding perhaps wasn’t quite there
in the beginning, but it is now. Academics should from time to time come up
with a new page 1 for their papers.&lt;/p&gt;

&lt;h3 id=&quot;build-reliable-code-repositories&quot;&gt;Build reliable code repositories&lt;/h3&gt;

&lt;p&gt;A  weakness of the differential privacy community has been the scarcity of
available high quality code. There are many academic code pieces available
by emailing someone, but we don’t have many visible repositories on github
or elsewhere that provide robust implementations of common differentially
private algorithms. Frank McSherry’s &lt;a href=&quot;http://research.microsoft.com/en-us/projects/pinq/&quot;&gt;PINQ&lt;/a&gt; was a really wonderful step in the right direction,
but it is no longer maintained and by now out of date. Written in C#, it hasn’t been easy for many to build on and extend PINQ. A more recent notable effort is the &lt;a href=&quot;https://github.com/ejgallego/dualquery&quot;&gt;Dual Query&lt;/a&gt; code though it requires CPLEX to run.&lt;/p&gt;

&lt;p&gt;What scares me a bit is that even a project as solidly designed and carefully executed as PINQ
did not address low-level implementation issues such as &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2382264&quot;&gt;floating point vulnerabilities&lt;/a&gt; in differential privacy.&lt;/p&gt;

&lt;p&gt;I’m guilty myself. Many have used or tried to use &lt;a href=&quot;http://papers.nips.cc/paper/4548-a-simple-and-practical-algorithm-for-differentially-private-data-release.pdf&quot;&gt;MWEM&lt;/a&gt;, an algorithm Katrina Ligett, Frank McSherry and I presented at NIPS a few years ago. Yet we don’t have a great implementation publicly available. You can email us for a decent C# implementation (alas!), but instead a lot of people have
produced their own implementations of our algorithm over the years. I regularly have the urge to start an open source project for it, but then I realize it’s a bit of a bottomless pit. In order to have a solid implementation of MWEM, I’d first need to have a solid implementation of all the primitives with all the low-level issues that come up. In any case, if somebody more brave then myself took the first step on an open source effort (preferably not in C#), I’d be very eager to contribute.&lt;/p&gt;

&lt;p&gt;Taking a more modest step, I feel compelled to compile a list of available code repositories.
If you have any pointers, please leave a comment!&lt;/p&gt;

&lt;h3 id=&quot;be-less-general-and-more-domain-specific&quot;&gt;Be less general and more domain-specific&lt;/h3&gt;

&lt;p&gt;Much of the academic research on differential privacy has focused on generality. That makes sense theoretically, but it means that reading the scientific literature on differential privacy from the point of
view of a domain expert can be very frustrating. Most papers start with toy
examples that make perfect sense on a theoretical level, but will appear
alarmingly naïve to a domain expert.&lt;/p&gt;

&lt;p&gt;The community is at a point where we need to transition &lt;em&gt;from generality to specificity&lt;/em&gt;.
For example, what’s needed are domain-specific tutorials
that walk practitioners through real examples. One reason why such
tutorials don’t exist is that they take a lot of time and writing them isn’t
incentivized by academia. One way out of this is for journal editors and
conferences to specifically invite such tutorials. Similarly, the community should at this point have very high regard for positive results and case studies in specific application domains even if they are limited in scope and don’t contribute technically new solutions.&lt;/p&gt;

&lt;h3 id=&quot;be-more-entrepreneurial&quot;&gt;Be more entrepreneurial&lt;/h3&gt;

&lt;p&gt;The CPUC case highlighted that the application of differential privacy in
practice can fail as a result of many non-technical issues. These important
issues are often not on the radar of academic researchers. We spent an awful
lot of time talking about the technical strengths or limitations of
differential privacy, while missing out on some very real challenges. It’s quite reasonable to argue
that these challenges  should be outside the scope of academia. On the other hand, academics are currently the
only available experts on differential privacy and there’s obvious demand for it.
Where should we draw the line?&lt;/p&gt;

&lt;p&gt;To be blunt, I think an important ingredient that’s missing in the current differential
privacy ecosystem is &lt;em&gt;money&lt;/em&gt;. There is only so much that academic researchers can do to promote a technology.
Beyond a certain point businesses have to commercialize the technology for it be successful. The CPUC
case was much better suited as the full-time job for a group of paid professionals
rather than a volunteering effort. I’m surprised none of the researchers working on differential privacy
have devoted a sabbatical to running a privacy startup. It’s needed and the potential upside is big.
Why not give it a shot? I hear tenured jobs are meant for running startups.&lt;/p&gt;

&lt;h3 id=&quot;so-is-differential-privacy-practical&quot;&gt;So, is differential privacy practical?&lt;/h3&gt;

&lt;p&gt;I like the answer Aaron Roth gave when I asked him:&lt;/p&gt;

&lt;div style=&quot;text-align:center;&quot;&gt;
&lt;em&gt;It&#39;s within striking distance.&lt;/em&gt;
&lt;/div&gt;

</description>
        <pubDate>Fri, 13 Mar 2015 15:30:00 -0700</pubDate>
        <link>http://blog.mrtz.org/2015/03/13/practicing-differential-privacy.html</link>
        <guid isPermaLink="true">http://blog.mrtz.org/2015/03/13/practicing-differential-privacy.html</guid>
        
        <category>tcs</category>
        
        <category>differential privacy</category>
        
        <category>practice</category>
        
        
      </item>
    
      <item>
        <title>Competing in a data science contest without reading the data</title>
        <description>&lt;p&gt;Machine learning competitions have become an extremely popular format for
solving prediction and classification problems of all sorts. The most famous
example is perhaps the Netflix prize. An even better example is
&lt;a href=&quot;http://www.kaggle.com&quot;&gt;Kaggle&lt;/a&gt;, an awesome startup that’s
organized more than a hundred competitions over the past few years.&lt;/p&gt;

&lt;p&gt;The central component of any competition is the public leaderboard. Competitors can repeatedly submit a list of predictions and see how their predictions perform on a set of &lt;em&gt;holdout labels&lt;/em&gt; not available to them. The leaderboard ranks all teams according to their prediction accuracy on the holdout labels. Once the competition closes all teams are scored on a final test set not used so far. The resulting ranking, called private leaderboard, determines the winner.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/heritage-pub.jpg&quot; alt=&quot;Heritage Prize public leaderboard&quot; /&gt;&lt;/p&gt;

&lt;div style=&quot;text-align:center;margin-bottom:10px&quot;&gt;
Public leaderboard of the Heritage Health Prize (&lt;a href=&quot;http://www.heritagehealthprize.com/c/hhp/leaderboard/public&quot;&gt;Source&lt;/a&gt;)
&lt;/div&gt;

&lt;p&gt;In this post, I will describe a method to climb the public leaderboard &lt;em&gt;without even looking at the data&lt;/em&gt;. The algorithm is so simple and natural that an unwitting analyst might just run it. We will see that in Kaggle’s famous Heritage Health Prize competition this might have propelled a participant from rank around 150 into the top 10 on the public leaderboard without making progress on the actual problem. The Heritage Health Prize competition ran for two years and had a prize pool of 3 million dollars. Keep in mind though that the standings on the public leaderboard do not affect who gets the money.&lt;/p&gt;

&lt;p&gt;The point of this post is to illustrate why maintaining a leaderboard that accurately reflects the true performance of each team is a difficult and deep problem. While there are decades of work on estimating the true performance of a model (or set of models) from a finite sample, the leaderboard application highlights some
challenges that while fundamental have only recently seen increased attention. A follow-up post will describe a &lt;a href=&quot;http://arxiv.org/abs/1502.04585&quot;&gt;recent paper&lt;/a&gt; with Avrim Blum that gives an algorithm for maintaining a (provably) accurate public leaderboard.&lt;/p&gt;

&lt;p&gt;Let me be very clear that my point is &lt;em&gt;not&lt;/em&gt; to criticize Kaggle or anyone else organizing machine learning competitions. On the contrary, I’m amazed by how well Kaggle competitions work. In my opinion, they have contributed a tremendous amount of value to both industry and education. I also know that Kaggle has some very smart people thinking hard about how to anticipate problems with competitions.&lt;/p&gt;

&lt;h2 id=&quot;the-kaggle-leaderboard-mechanism&quot;&gt;The Kaggle leaderboard mechanism&lt;/h2&gt;

&lt;p&gt;At first sight, the Kaggle mechanism looks like the classic &lt;em&gt;holdout method&lt;/em&gt;. Kaggle partitions the data into two sets: a training set and a holdout set. The training set is publicly available with both the individual instances and their corresponding class labels. The instances of the holdout set are publicly available as well, but the class labels are withheld. Predicting these missing class labels is the goal of the participant and a valid submission is a list of labels—one for each point in the holdout set.&lt;/p&gt;

&lt;p&gt;Kaggle specifies a score function that maps a submission consisting of N labels to a numerical score, which we assume to be in [0,1]. Think of the score as prediction error (smaller is better). For concreteness, let’s fix it to be the &lt;em&gt;misclassification rate&lt;/em&gt;. That is a prediction incurs loss 0 if it matches the corresponding unknown label and loss 1 if it does not match it. We divide by the number of predictions to get a score in [0,1].&lt;/p&gt;

&lt;p&gt;Kaggle further splits its \(N\) private labels randomly into \(n\) holdout labels and \(N-n\) test labels. Typically, \(n=0.3N\). The public leaderboard is a sorting of all teams according to their score computed only on the \(n\) holdout labels (without using the test labels), while the private leaderboard is the ranking induced by the test labels.  I will let \(s_H(y)\) denote the public score of a submission \(y\), i.e., the score according to the public leaderboard. Typically, Kaggle rounds all scores to 5 or 6 digits of precision.&lt;/p&gt;

&lt;h2 id=&quot;the-cautionary-tale-of-wacky-boosting&quot;&gt;The cautionary tale of wacky boosting&lt;/h2&gt;

&lt;p&gt;Imagine your humble blogger in a parallel universe: I’m new to this whole machine learning craze. So, I sign up for a Kaggle competition to get some skills. Kaggle tells me that there’s an unknown set of labels \(y\in\{0,1\}^N\) that I need to predict. Well, I know nothing about \(y\). So here’s what I’m going to do. I try out a bunch of random vectors and keep all those that give me a slightly better than expected score. If we’re talking about misclassification rate, the expected score of a random binary vector is 0.5. So, I’m keeping all the vectors with score less than 0.5. Then I recall something about boosting. It tells me that I can boost my accuracy by aggregating all predictors into a single predictor using the majority function. Slightly more formally, here’s what I do:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Algorithm&lt;/strong&gt; (Wacky Boosting):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Choose \(y_1,\dots,y_k\in\{0,1\}^N\) uniformly at random.&lt;/li&gt;
  &lt;li&gt;Let \(I = \{ i\in[k] \colon s_H(y_i) &amp;lt; 0.5 \}\).&lt;/li&gt;
  &lt;li&gt;Output \(\hat y=\mathrm{majority} \{ y_i \colon i \in I \} \), where the majority is component-wise.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Lo and behold, this is what happens:&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;object data=&quot;/assets/boosting.svg&quot; type=&quot;image/svg+xml&quot;&gt;
&lt;param name=&quot;src&quot; value=&quot;/assets/boosting.svg&quot; /&gt;
  &lt;img src=&quot;/assets/boosting.png&quot; /&gt;
&lt;/object&gt;
&lt;p style=&quot;text-align:center&quot;&gt;In this plot, \(n=4000\) and all numbers are averaged over 5 independent repetitions.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;As I’m only seeing the public score (bottom red line), I get super excited. I keep climbing the leaderboard! Who would’ve thought that this machine learning thing was so easy? So, I go write a blog post on Medium about Big Data and score a job at DeepCompeting.ly, the latest data science startup in the city. Life is pretty sweet. I pick up indoor rock climbing, sign up for wood working classes; I read Proust and books about espresso. Two months later the competition closes and Kaggle releases the final score. What an embarrassment! Wacky boosting did nothing whatsoever on the final test set. I get fired from DeepCompeting.ly days before the buyout. My spouse dumps me. The lease expires. I get evicted from my apartment in the Mission. Inevitably, I hike the Pacific Crest Trail and write a novel about it.&lt;/p&gt;

&lt;h3 id=&quot;what-just-happened&quot;&gt;What just happened&lt;/h3&gt;

&lt;p&gt;Let’s understand what went wrong and how you can avoid hiking the Pacific Crest Trail. To start out with, each \(y_i\) has loss around \(1/2\pm1/\sqrt{n}\). We’re selecting the ones that are biased below a half. This introduces a bias in the score and the conditional expected bias of each selected vector \(w_i\) is roughly \(1/2-c/\sqrt{n}\) for some positive constant \(c&amp;gt;0\). Put differently, each selected \(y_i\) is giving us a guess about each label in the unknown holdout set \(H\subseteq [N]\) that’s correct with probability \(1/2 + \Omega(1/\sqrt{n})\). Since the public score doesn’t depend on labels outside of \(H\), the conditioning does not affect the final test set. The labels outside of \(H\) are still unbiased. Finally, we need to argue that the majority vote “boosts” our slightly biased coin tosses into a stronger bias. More formally, we can show that \(\hat y\) gives us a guess for each label in \(H\) that’s correct with probability
\[
\frac12 + \Omega\left(\sqrt{k/n}\right).
\]
Hence, the public score of \(y\) satisfies
\[
s_H(y) &amp;lt; \frac12 - \Omega\left(\sqrt{k/n}\right).
\]
Outside of \(H\), however, we’re just random guessing with no advantage.
To summarize, wacky boosting gives us &lt;em&gt;a bias of \(\sqrt{k}\) standard deviations on the public score with \(k\) submissions&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;What’s important is that the same algorithm still “works” even if we don’t get exact answers. All we need are answers that are accurate to an additive error of \(1/\sqrt{n}\). This is important since Kaggle rounds its answers to 5 digits of precision. In particular, this attack will work so long as \(n&amp;lt; 10^{10}\).&lt;/p&gt;

&lt;h3 id=&quot;why-the-holdout-method-breaks-down&quot;&gt;Why the holdout method breaks down&lt;/h3&gt;

&lt;p&gt;The idea behind the holdout method is that the holdout data serve as a fresh sample providing an unbiased and well-concentrated estimate of the true loss of the classifier on the underlying distribution. Why then didn’t the holdout method detect that our wacky boosting algorithm was overfitting? The short answer is that the holdout method is simply not valid in the way it’s used in a competition.&lt;/p&gt;

&lt;p&gt;One point of departure from the classic method is that the participants actually do see the data points corresponding to holdout labels which can lead to some problems. But that’s not the issue here and even if they we don’t look at the holdout data points at all, there’s a fundamental reason why the validity of the classic holdout method breaks down.&lt;/p&gt;

&lt;p&gt;The problem is that a submission in general incorporates information about the holdout labels previously released through the leaderboard mechanism. As a result, &lt;strong&gt;there is a statistical dependence between the holdout data and the submission&lt;/strong&gt;. Due to this feedback loop, the public score is in general no longer an unbiased estimate of the true score. There is no reason not to expect the submissions to eventually overfit to the holdout set.&lt;/p&gt;

&lt;p&gt;The problem of overfitting to the holdout set is well known. Kaggle’s forums are full of anecdotal evidence reported by various competitors. The primary way Kaggle deals with this problem is by limiting the rate of re-submission and (to some extent) the bit precision of the answers. Of course, this is also the reason why the winners are determined on a separate test set.&lt;/p&gt;

&lt;h3 id=&quot;static-vs-interactive-data-analysis&quot;&gt;Static vs interactive data analysis&lt;/h3&gt;

&lt;p&gt;Kaggle’s liberal use of the holdout method is just one example of a widespread disconnect between the theory of &lt;strong&gt;static data analysis&lt;/strong&gt; and the practice of &lt;strong&gt;interactive data analysis&lt;/strong&gt;. The holdout method is a static method in that it assumes the model to be independent of the holdout data on which it is evaluated. However, machine learning competitions are interactive, because submissions generally incorporate information from the holdout set.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/staticvsint.jpg&quot; alt=&quot;Static vs Interactive&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I contend that most real world data analysis is interactive. Unfortunately, most of the theory on model validation and statistical estimation falls into the static setting requiring independence between method and holdout data. This divide is &lt;em&gt;not&lt;/em&gt; inherent though. Indeed, my next post deals with some useful theory for the interactive setting.&lt;/p&gt;

&lt;h2 id=&quot;the-heritage-health-prize-leaderboard&quot;&gt;The Heritage Health Prize leaderboard&lt;/h2&gt;

&lt;p&gt;Let’s see how this could’ve been applied to an actual competition. Of course, the &lt;a href=&quot;http://www.heritagehealthprize.com/c/hhp&quot;&gt;Heritage Health Prize&lt;/a&gt; competition is long over. We’re about two years too late to the party. Besides, we don’t have the solution file for that competition. Without it there’s no sure way of knowing how well this approach would’ve worked. Nevertheless, we can make some reasonable modeling of what the holdout labels might look like using information that was released by Kaggle and see how well we’d be doing against our random model.&lt;/p&gt;

&lt;h3 id=&quot;generalized-wacky-boosting&quot;&gt;Generalized wacky boosting&lt;/h3&gt;

&lt;p&gt;Before we can apply wacky boosting to the Heritage prize, we need to clear two obstacles.
First, wacky boosting required the domain to be Boolean whereas the labels could be arbitrary positive real numbers. Second, the algorithm only gave an advantage over random guessing which might be too far from the top of the leaderboard to start out with. It turns out that both of these issues can be resolved nicely with a simple generalization of the previous algorithm. What was really happening in the algorithm is that we had two candidate solutions, the all ones vector and the all zeros vector, and we tried out random coordinate-wise combinations of these vectors. The algorithm ends up finding a coordinate wise combination of the two vectors that improves upon their mean loss, i.e., one half. This way of looking at it generalizes nicely. The resulting algorithm is just a few lines of Julia code.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;&lt;span class=&quot;c&quot;&gt;# select coordinate from v1 where v is 1 and from v2 where v is 0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;combine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt; wackyboost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# select columns of A that give better than mean score&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;combine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# take majority vote over all selected columns&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;combine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I worked through the fun exercise of applying this algorithm in a separate &lt;a href=&quot;http://nbviewer.ipython.org/gist/mrtzh/c41fd4c5897fc114a0d6&quot;&gt;Julia notebook&lt;/a&gt;. Here’s one picture that came out of it. Don’t treat the numbers as definitive as they all depend on the modeling assumptions I made.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;object data=&quot;/assets/heritage.svg&quot; type=&quot;image/svg+xml&quot;&gt;
  &lt;img src=&quot;/assets/heritage.png&quot; /&gt;
&lt;/object&gt;
&lt;p style=&quot;text-align:center&quot;&gt;We see an improvement from 0.462311 (rank 146) to 0.451868  (rank 6).&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The bottom line is: It seems to work reasonably well (under various semi-principled modeling assumptions I made). From the looks of it this might have given you an improvement &lt;em&gt;from rank 150ish to 6ish&lt;/em&gt; within 700 submissions. Note there was a single team with 671 submissions. There’s a pretty good gap between number one on the &lt;a href=&quot;http://www.heritagehealthprize.com/c/hhp/leaderboard/public&quot;&gt;public leaderboard&lt;/a&gt; and the rest. While possible in principle, it took me a bunch more submissions to get to the top. I should say though that I used the completely generic code from above without any optimizations specific to the competition. I didn’t even look at the data points (as I don’t have them). It’s possible that using the data and domain knowledge could improve things much further. I chose the Heritage Health prize, because it was the highest prized Kaggle competition ever (3 million dollars) and it ran for two years with a substantial number of submissions.&lt;/p&gt;

&lt;h2 id=&quot;how-robust-is-your-benchmark&quot;&gt;How robust is your benchmark?&lt;/h2&gt;

&lt;p&gt;There’s a broad lesson to be learned from this example. As computer scientists we love numerical benchmarks and rankings of all sorts. They look so objective and scientific that we easily forget how any benchmark is just a proxy for a more complex question. Every once in a while we should step back and ask: How robust is the benchmark? Do improvements in our benchmark really correspond to progress on the original problem? What I’d love to see in all empirical branches of computer science are adversarial robustness evaluations of various benchmarks. How far can we get by &lt;em&gt;gaming&lt;/em&gt; rather than by actually making progress towards solving the problem?&lt;/p&gt;

&lt;p&gt;Let me end on a positive note. What excites me is that the serious problems we saw in this post actually do have a fix (both in theory and in practice)! So, stay tuned for my next post.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Subscribe to the  &lt;a href=&quot;http://blog.mrtz.org/feed.xml&quot;&gt;RSS feed&lt;/a&gt;
or follow me on &lt;a href=&quot;https://www.twitter.com/mrtz&quot;&gt;Twitter&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Mar 2015 10:30:00 -0700</pubDate>
        <link>http://blog.mrtz.org/2015/03/09/competition.html</link>
        <guid isPermaLink="true">http://blog.mrtz.org/2015/03/09/competition.html</guid>
        
        <category>tcs</category>
        
        
      </item>
    
      <item>
        <title>Goodbye Wordpress, never again</title>
        <description>&lt;p&gt;Some may have noticed that this blog was an utter mess for about two weeks.
This was due to an exploited vulnerability in Wordpress leaving my blog 
in a state most curiously difficult to recover from. The only thing not
corrupted by this horrific cyberattack (possibly of North Korean origin) 
was an SQL database dump of my blog
that turned out to be inconsistent with a fresh Wordpress install. The
Wordpress support team thankfully pointed me to a number of videos
explaning carefully how to move my mouse
cursor from one point to another. I feel a lot more confident now with the mouse
cursor. In the
end, I more or less manually moved my blog to &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt; hosted by
&lt;a href=&quot;https://pages.github.com&quot;&gt;GitHub&lt;/a&gt; with &lt;a href=&quot;http://disqus.com&quot;&gt;Disqus&lt;/a&gt; comments.
It’s a pretty decent combination that a lot of people seem to be transitioning
to. &lt;/p&gt;

&lt;p&gt;If you ever think about starting a blog, &lt;em&gt;do not&lt;/em&gt; go with Wordpress. If you absolutely
cannot resist the temptation, by all means do not maintain your own Wordpress
install. Wordpress is slow, clunky and insecure. I had to install updates
multiple times a month and even that was evidently not enough to stay on top.&lt;/p&gt;

&lt;p&gt;On a positive note, I appear to be on paternity leave right now—for another
five weeks or so—and I hope to write a blog post or two in those moments
when I’m not spending quality time with my bicycles, ehm, and my daughter, of
course.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;To stay on top of future posts, subscribe to the (new) &lt;a href=&quot;http://blog.mrtz.org/feed.xml&quot;&gt;RSS feed&lt;/a&gt;
or follow me on &lt;a href=&quot;https://www.twitter.com/mrtz&quot;&gt;Twitter&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 08 Feb 2015 11:45:39 -0800</pubDate>
        <link>http://blog.mrtz.org/2015/02/08/wordpress.html</link>
        <guid isPermaLink="true">http://blog.mrtz.org/2015/02/08/wordpress.html</guid>
        
        <category>tcs</category>
        
        
      </item>
    
      <item>
        <title>The NIPS Experiment</title>
        <description>&lt;p&gt;&lt;em&gt;This is a guest post by &lt;a href=&quot;http://cs.utexas.edu/~ecprice/&quot;&gt;Eric Price&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I was at NIPS (one of the two main machine learning conferences) in Montreal last week, which has a really advanced format relative to the theory conferences. The double blind reviewing, rebuttal phase, and poster+lightning talk system all seem like improvements on the standard in my normal area (theoretical computer science), and having 2400 attendees is impressive and overwhelming. But the most amazing thing about the conference organization this year was the &lt;a href=&quot;http://inverseprobability.com/2014/12/16/the-nips-experiment/&quot;&gt;NIPS consistency experiment&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A perennial question for academics is how accurate the conference review and acceptance process is. Getting papers into top conferences is hugely important for our careers, yet we all have papers rejected that we think should have gotten in. One of my papers was rejected three times before getting into SODA – as the best student paper. After rejections, we console ourselves that the reviewing process is random; yet we take acceptances as confirmation that our papers are good. So just how random &lt;em&gt;is&lt;/em&gt; the reviewing process?  The NIPS organizers decided to find out.&lt;/p&gt;

&lt;h2 id=&quot;the-nips-experiment&quot;&gt;The NIPS Experiment&lt;/h2&gt;

&lt;p&gt;The NIPS consistency experiment was an amazing, courageous move by the organizers this year to quantify the randomness in the review process. They split the program committee down the middle, effectively forming two independent program committees. Most submitted papers were assigned to a single side, but 10% of submissions (166) were reviewed by &lt;em&gt;both&lt;/em&gt; halves of the committee. This let them observe how consistent the two committees were on which papers to accept.  (For fairness, they ultimately accepted any paper that was accepted by either committee.)&lt;/p&gt;

&lt;p&gt;The results were revealed this week: of the 166 papers, the two committees disagreed on the fates of 25.9% of them: 43. [Update: the original post said 42 here, but I misremembered.] But this “25%” number is misleading, and most people I’ve talked to have misunderstood it: it actually means that the two committees &lt;em&gt;disagreed more than they agreed&lt;/em&gt; on which papers to accept. Let me explain.&lt;/p&gt;

&lt;p&gt;The two committees were each tasked with a 22.5% acceptance rate. This would mean choosing about 37 or 38 of the 166 papers to accept&lt;sup&gt;&lt;a href=&quot;#footnotes&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Since they disagreed on 43 papers total, this means one committee accepted 21 papers that the other committee rejected and the other committee accepted 22 papers the first rejected, for 21 + 22 = 43 total papers with different outcomes. Since they accepted 37 or 38 papers, this means they disagreed on 21/37 or 22/38 ≈ 57% of the list of accepted papers.&lt;/p&gt;

&lt;p&gt;In particular, about 57% of the papers accepted by the first committee were rejected by the second one and vice versa. In other words, most papers at NIPS would be rejected if one reran the conference review process (with a 95% confidence interval of 40-75%):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/nips-pie11.png&quot; alt=&quot;Nips pie chart&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center&quot;&gt;Most papers accepted by one committee were rejected by the other, and vice versa.&lt;/p&gt;

&lt;p&gt;This result was surprisingly large to most people I’ve talked to; they generally expected something like 30% instead of 57%. Relative to what people expected, 57% is actually closer to a purely random committee, which would only disagree on 77.5% of the accepted papers on average:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/randomgraph2.png&quot; alt=&quot;Random model&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center&quot;&gt;
If the committees were purely random, at a 22.5%
acceptance rate&lt;br /&gt; they would disagree on 77.5% of their acceptance lists on average.&lt;/p&gt;

&lt;p&gt;In the next section, I’ll discuss a couple simple models for the conference review process that give the observed level of randomness.&lt;/p&gt;

&lt;h2 id=&quot;models-for-conference-acceptance&quot;&gt;Models for conference acceptance&lt;/h2&gt;

&lt;p&gt;One rough model for paper acceptance, consistent with the experiment, is as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Half the submissions are found to be poor and reliably rejected.&lt;/li&gt;
  &lt;li&gt;The other half are accepted based on an unbiased coin flip.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This might be a decent rule of thumb, but it’s clearly missing something: some really good papers do have a chance of acceptance larger than one half.&lt;/p&gt;

&lt;h3 id=&quot;the-messy-middle-model&quot;&gt;The “messy middle” model&lt;/h3&gt;

&lt;p&gt;One simple extension to the above model is the “messy middle” model, where some papers are clear accepts; some papers are clear rejects; and the papers in the middle are largely random.   We can compute what kinds of parameters are consistent with the NIPS experiment.  Options include:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;The above model.&lt;/strong&gt; Half the papers are clear rejects, and everything else is random.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The opposite.&lt;/strong&gt; 7% of all papers (i.e. 30% of accepted papers) are clear accepts, and the  other 93% are random.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Somewhere in the middle.&lt;/strong&gt; For example 6% of all papers (i.e. 25% of accepted papers) are clear accepts, 25% of submitted papers are clear rejects, and the rest are random.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/messymiddle.png&quot; alt=&quot;Messy middle&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-noisy-scoring-model&quot;&gt;The “noisy scoring” model&lt;/h3&gt;

&lt;p&gt;As I was discussing this over dinner, Jacob Abernethy proposed a “noisy scoring” model based on his experience as an area chair. Each paper typically gets three reviews, each giving a score on 0-10. The committee uses the average score&lt;sup&gt;&lt;a href=&quot;#footnotes&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; as the main signal for paper quality. As I understand it, the basic committee process was that almost everything above 6.5 was accepted, almost everything below 6 was rejected, and the committee mainly debated the papers in between.&lt;/p&gt;

&lt;p&gt;A basic simplified model of this would be as follows. Each paper has a “true”
score \(v\) drawn from some distribution (say, \({N(0,
\sigma_{between}^2)}\)), and the the reviews for the paper are drawn
from \(N(v, \sigma_{within}^2)\). Then the NIPS experiment’s
result (number of papers in which the two committees disagree) is a function
of the ratio \(\sigma_{between}/\sigma_{within}\). We find that
the observation would be consistent with this model if
\(\sigma_{within}\) is between one and four times \(\sigma_{between}\):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/noisyscoring2.png&quot; alt=&quot;Noisy scoring&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once the NIPS review data is released, we can check the empirical
\({\sigma_{within}}\) and \({\sigma_{between}}\) to see if this model is reasonable.&lt;/p&gt;

&lt;p&gt;One nice thing about the noisy scoring model is that you don’t actually need to run the NIPS experiment to estimate the parameters. Every CS conference could measure the within-paper and between-paper variance in reviewer scores. This lets you measure the expected randomness in the results of the process, assuming the model holds.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Computer science conference acceptances seem to be more random than we had previously realized. This suggests that we should rethink the importance we give to them in terms of the job search, tenure process, etc.&lt;/p&gt;

&lt;p&gt;I’ll close with a few final thoughts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Consistency is not the only goal. Double-blind reviewing probably decreases consistency by decreasing the bias towards established researchers, but this is a good thing and the TCS conferences should adopt the system.&lt;/li&gt;
  &lt;li&gt;Experiments are good! As scientists, we ought to do more experiments on our processes. The grad school admissions process seems like a good target for this, for example.&lt;/li&gt;
  &lt;li&gt;I’d like to give a &lt;em&gt;huge&lt;/em&gt; shout-out to the NIPS organizers, Corinna Cortes and Neil Lawrence, for running this experiment. It wasn’t an easy task – not only did they review 10% more papers than necessary, they also had the overhead of finding and running two independent PCs. But the results are valuable for the whole computer science community.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;The committees did not know which of the ~900 papers they were reviewing were the 166 duplicated ones, so there can be some variation in how many papers to accept, but this is a minor effect.&lt;/li&gt;
  &lt;li&gt;They also use a “confidence-weighted” score, but let’s ignore that detail.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 15 Dec 2014 11:45:39 -0800</pubDate>
        <link>http://blog.mrtz.org/2014/12/15/the-nips-experiment.html</link>
        <guid isPermaLink="true">http://blog.mrtz.org/2014/12/15/the-nips-experiment.html</guid>
        
        <category>tcs</category>
        
        
      </item>
    
      <item>
        <title>How big data is unfair</title>
        <description>&lt;p&gt;Head on over to &lt;a href=&quot;https://medium.com/@mrtz/how-big-data-is-unfair-9aa544d739de&quot;&gt;Medium&lt;/a&gt;
for a non-technical general audience piece I wrote on &lt;em&gt;why machine learning is not, by
default, fair or just in any meaningful way.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Since I first wrote this post there’s been some interesting follow up by
&lt;a href=&quot;https://medium.com/@hannawallach/big-data-machine-learning-and-the-social-sciences-927a8e20460d&quot;&gt;Hanna
Wallach&lt;/a&gt;.
Also be sure to check out the web site of the &lt;a href=&quot;http://www.fatml.org&quot;&gt;NIPS workshop&lt;/a&gt; on fairness,
accountability and transparency that Solon Barocas and I organized.&lt;/p&gt;
</description>
        <pubDate>Fri, 26 Sep 2014 18:02:37 -0700</pubDate>
        <link>http://blog.mrtz.org/2014/09/26/how-big-data-is-unfair.html</link>
        <guid isPermaLink="true">http://blog.mrtz.org/2014/09/26/how-big-data-is-unfair.html</guid>
        
        
      </item>
    
      <item>
        <title>Robustness versus Acceleration</title>
        <description>&lt;p&gt;My blog post on the &lt;a href=&quot;http://mrtz.org/blog/the-zen-of-gradient-descent/&quot;&gt;Zen of Gradient Descent&lt;/a&gt; hit the front page of &lt;a href=&quot;https://news.ycombinator.com/item?id=8182991&quot;&gt;Hacker News&lt;/a&gt; the other day. I don&#39;t know how that happened. It got me more views in one day than this most humble blog usually gets in half a year. I thought I should take this as an excuse to extend the post a bit by elaborating on one remark I made only in passing. You don&#39;t need to go back to reading that post unless you want to. This one will be self contained.&lt;/p&gt;
&lt;p&gt;The point I made is that basic Gradient Descent (GD) is noise tolerant in a way that Accelerated Gradient Descent (AGD) is not. That is to say, if we don&#39;t have exact but rather approximate gradient information, GD might very well outperform AGD even though its convergence rate is worse in the exact setting. The truth is I was sort of bluffing. I didn&#39;t actually have a proof of a formal statement that would nail down this point in a compelling way. It was more of a gut feeling based on some simple observations.&lt;/p&gt;
&lt;p&gt;To break the suspense, I still haven&#39;t proved the statement I vaguely thought was true back then, but fortunately somebody else had already done that. This is a thought provoking &lt;a href=&quot;http://www.optimization-online.org/DB_FILE/2010/12/2865.pdf&quot;&gt;paper by Devolder, Glineur and Nesterov &lt;/a&gt;(DGN). Thanks to Cristobal Guzman for pointing me to this paper. Roughly, what they show is that any method that converges faster than the basic gradient descent method must accumulate errors linearly with the number of iterations. Hence, in various noisy settings auch as are common in applications acceleration may not help---in fact, it can actually make things worse!&lt;/p&gt;
&lt;p&gt;I&#39;ll make this statement more formal below, but let me first explain why I love this result. There is a tendency in algorithm design to optimize computational efficiency first, second and third and then maybe think about some other stuff as sort of an add-on constraint. We generally tend to equate faster with better. The reason why this is not a great methodology is that sometimes acceleration is mutually exclusive with other fundamental design goals. A theory that focuses primarily on speedups without discussing trade-offs with robustness misses a pretty important point.&lt;/p&gt;
&lt;h2&gt;When acceleration is good&lt;/h2&gt;
&lt;p&gt;Let&#39;s build some intuition for the result I mentioned before we go into formal details. Consider my favorite example of minimizing a smooth convex function \({f\colon \mathbb{R}^n\rightarrow\mathbb{R}}\) defined as&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle f(x) = \frac 12 x^T L x - b^T x \)&lt;/p&gt;
&lt;p&gt;for some positive semidefinite \({n\times n}\) matrix \({L}\) and a vector \({b\in\mathbb{R}^n.}\) Recall that the gradient is \({\nabla f(x)=Lx-b.}\) An illustrative example is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Laplacian_matrix&quot;&gt;Laplacian &lt;/a&gt; of a &lt;a href=&quot;http://en.wikipedia.org/wiki/Cycle_graph&quot;&gt;cycle graph&lt;/a&gt;:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle L = \left[ \begin{array}{ccccccc} 2 &amp;amp; -1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; -1 \\ -1 &amp;amp; 2 &amp;amp; -1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ 0 &amp;amp; -1 &amp;amp; 2 &amp;amp; -1&amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ \vdots &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; \end{array} \right] \)&lt;/p&gt;
&lt;p&gt;Since \({L}\) is positive semidefinite like any graph Laplacian, the function \({f}\) is convex. The operator norm of \({L}\) is bounded by~\({4}\) and so we have that for all \({x,y\in\mathbb{R}^n:}\)&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle \|\nabla f(x) -\nabla f(y)\| \le \|L\|\cdot\|x-y\|\le 4 \|x-y\|. \)&lt;/p&gt;
&lt;p&gt;This means the function is also smooth and we can apply AGD/GD with a suitable step size. Comparing AGD and GD on this instance with \({n=100}\), we get the following picture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/no-noise.png&quot; alt=&quot;No noise&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It looks like AGD is the clear winner. GD is pretty slow and takes a few thousand iterations to decrease the error by an order of magnitude.&lt;/p&gt;
&lt;h2&gt;When acceleration is bad&lt;/h2&gt;
&lt;p&gt;The situation changes dramatically in the presence of noise. Let&#39;s repeat the exact same experiment but now instead of observing \({\nabla f(x)}\) for any given \({x,}\) we can only see \({\nabla f(x) + \xi}\) where \({\xi}\) is sampled from the \({n}\)-dimensional normal distribution \({N(0,\sigma^2)^n.}\) Choosing \({\sigma=0.1}\) we get the following picture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/with-noise.png&quot; alt=&quot;With noise&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Gradient descent pretty quickly converges to essentially the best result that we can hope for given the noisy gradients. In contrast, AGD goes totally nuts. It doesn&#39;t converge at all and it adds up errors in sort of linear fashion. In this world, GD is the clear winner.&lt;/p&gt;

&lt;h2&gt;A precise trade-off&lt;/h2&gt;
&lt;p&gt;The first thing DGN do in their paper is to define a general notion of inexact first order oracle. Let&#39;s recall what an exact first-order oracle does for an (unconstrained) convex function \({f\colon\mathbb{R}^n\rightarrow \mathbb{R}}\) with smoothness parameter \({L.}\) Given any point \({x\in\mathbb{R}^n}\) an exact first order oracle returns a pair \({(f_L(x),g_L(x))\in\mathbb{R}\times\mathbb{R}^n}\) so that for all \({y\in\mathbb{R}^n}\) we have&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle 0\le f(y) - \big(f_L(x) + \langle g_L(x),y-x\rangle\big)\le \frac L2\|y-x\|^2\,. \)&lt;/p&gt;
&lt;p&gt;Pictorially, at every point \({x}\) the function can be sandwiched between a tangent linear function specified by \({(f_L(x),g_L(x))}\) and a parabola. The pair \({(f(x),\nabla f(x))}\) satisfies this constraint as the first inequality follows from convexity and the second from the smoothness condition. In fact, this pair is the only pair that satisfies these conditions. My slightly cumbersome way of desribing a first-order oracle was only so that we may now easily generalize it to an inexact first-order oracle. Specifically, an inexact oracle returns for any given point \({x\in\mathbb{R}^n}\) a pair \({(f_{\delta,L}(x),g_{\delta,L}(x))\in\mathbb{R}\times\mathbb{R}^n}\) so that for all \({y\in\mathbb{R}^n}\) we have&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle 0\le f(y) - \big(f_{\delta,L}(x) + \langle g_{\delta,L}(x),y-x\rangle\big)\le \frac L2\|y-x\|^2+\delta\,. \)&lt;/p&gt;
&lt;p&gt;It&#39;s the same picture as before except now there&#39;s some \({\delta}\) slack between the linear approximation and the parabola.&lt;br /&gt;
With this notion at hand, what DGN show is that given access to \({\delta}\)-inexact first-order oracle Gradient Descent spits out a point \({x^t}\) after \({t}\) steps so that&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle f(x^t) - \min_x f(x) \le O\big(L/t\big) + \delta\,. \)&lt;/p&gt;
&lt;p&gt;The big-oh notation is hiding the squared distance between the optimum and the starting point. Accelerated Gradient Descent on the other hand gives you&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle f(x^t) - \min_x f(x) \le O\big(L/t^2\big) + O\big(t \delta\big)\,. \)&lt;/p&gt;
&lt;p&gt;Moreover, you cannot improve this-tradeoff between acceleration and error accumulation. That is any method that converges as \({1/t^2}\) must accumulate errors as \({t\delta.}\)&lt;/p&gt;
&lt;h2&gt;Open questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The lower bound I just mentioned in the previous paragraph stems from the fact that an inexact first-order oracle can embed non-smooth optimization problems for which a speedup is not possible. This is interesting, but it doesn&#39;t resolve, for example, the question of whether there could be a speedup in the simple gaussian noise addition model that I mentioned above. This isn&#39;t even a toy model---as you might object---since gaussian noise addition is what you would do to make gradient descent privacy preserving. See for example an upcoming FOCS &lt;a href=&quot;http://arxiv.org/pdf/1405.7085.pdf&quot;&gt;paper by Bassily, Smith, Thakurta&lt;/a&gt; for an analysis of gradient descent with gaussian noise.&lt;/li&gt;
&lt;li&gt;Is there an analog of the DGN result in the eigenvalue world? More formally, can we show that any Krylov subspace method that converges asymptotically faster than the &lt;a href=&quot;http://mrtz.org/blog/power-method/&quot;&gt;power method&lt;/a&gt; must accumulate errors?&lt;/li&gt;
&lt;li&gt;The cycle example above is often used to show that any blackbox gradient method requires at least \({t\ge \Omega(1/\sqrt{\epsilon})}\) steps to converge to error \({\epsilon}\) provided that \({t}\) is less than the number of vertices of the cycle, that is the dimension \({n}\) of the domain. (See, for example, Theorem 3.9. in Sebastien Bubeck&#39;s &lt;a href=&quot;http://arxiv.org/pdf/1405.4980v1.pdf&quot;&gt;book&lt;/a&gt;.) Are there any lower bounds that hold for \({t\gg n}\)?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Pointers:&lt;/h2&gt;
&lt;p&gt;The code for these examples is available &lt;a href=&quot;http://nbviewer.ipython.org/gist/mrtzh/4dc77fb84c3ba8b8b220&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;To stay on top of future posts, subscribe to the &lt;a style=&quot;color: #bc360a;&quot; href=&quot;http://mrtz.org/blog/feed/&quot;&gt;RSS feed&lt;/a&gt; or follow me on &lt;a style=&quot;color: #bc360a;&quot; href=&quot;http://twitter.com/mrtz&quot;&gt;Twitter&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 18 Aug 2014 12:31:57 -0700</pubDate>
        <link>http://blog.mrtz.org/2014/08/18/robustness-versus-acceleration.html</link>
        <guid isPermaLink="true">http://blog.mrtz.org/2014/08/18/robustness-versus-acceleration.html</guid>
        
        <category>algorithms</category>
        
        <category>tcs</category>
        
        <category>complexity</category>
        
        <category>theory</category>
        
        <category>gradient descent</category>
        
        <category>convexity</category>
        
        <category>optimization</category>
        
        <category>robustness</category>
        
        
      </item>
    
      <item>
        <title>Pearson&#39;s polynomial</title>
        <description>&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;A 120 year old algorithm for learning mixtures of gaussians turns out to be optimal&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So, how was math writing in 1894? I’d imagined it to be a lot like one of Nietzsche’s last diary entries except with nouns replaced by German fraktur symbols, impenetrable formalisms and mind-bending passive voice constructions. I was in for no small surprise when I started reading Karl Pearson’s &lt;a href=&quot;https://archive.org/details/philtrans02543681&quot;&gt;Contributions to the Mathematical Theory of Evolution&lt;/a&gt;. The paper is quite enjoyable! The math is rigorous yet not overly formal. Examples and philosophical explanations motivate various design choices and definitions. A careful experimental evaluation gives credibility to the theory.&lt;/p&gt;

&lt;p&gt;The utterly mind-boggling aspect of the paper however is not how well-written it is, but rather what Pearson actually did in it and how far ahead of his time he was. This is the subject of this post.&lt;/p&gt;

&lt;p&gt;Pearson was interested in building a mathematical theory for evolutionary
biology. In hindsight, his work is also one of the earliest contributions to
the &lt;em&gt;computational&lt;/em&gt; theory of evolution. This already strikes me as
visionary as it remains a hot research area today. The Simons Institute in
Berkeley just devoted a &lt;a href=&quot;http://simons.berkeley.edu/programs/evolution2014&quot;&gt;semester-long program&lt;/a&gt; to it.&lt;/p&gt;

&lt;p&gt;In his paper, he considers the distribution of a single measurement among a population, more concretely, the forehead width to body length ratio among a population of crabs. The crab data had been collected by zoologist Weldon and his wife during their 1892 &lt;a href=&quot;http://statprob.com/encyclopedia/WalterFrankRaphaelWeldon.html&quot;&gt;easter vacation&lt;/a&gt; on Malta. Wikipedia describes the crab (&lt;a href=&quot;http://en.wikipedia.org/wiki/Carcinus_maenas&quot;&gt;carcinus maenas&lt;/a&gt;) as one of the &quot;&lt;em&gt;world&#39;s worst alien invasive species&quot;&lt;/em&gt;, but fails to acknowledge the crab&#39;s academic contributions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/crab1.jpg&quot; alt=&quot;Pearson-Weldon crab&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;The Weldons had taken 1000 samples each with 23 attributes of wich all but the one attribute describing forehead to body length ratio seemed to follow a single normal distribution. Regarding this peculiar deviation from normal, Pearson notes:&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;In the case of certain biological, sociological, and economic
measurements there is, however, a well-marked deviation from this normal
shape, and it becomes important to determine the direction and amount of such
deviation. The asymmetry may arise from the fact that the units grouped
together in the measured material are not really homogeneous. It may happen
that we have a mixture of \(2, 3,\dots, n\) homogeneous groups, each of which deviates about its own mean symmetrically and in a manner represented with sufficient accuracy by the normal curve.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;What the paragraph describes is the problem of &lt;em&gt;learning a mixture of gaussians&lt;/em&gt;: Each sample is drawn from one of several unknown gaussian distributions. Each distribution is selected with an unknown mixture probability. The goal is to find the parameters of each gaussian distribution as well as the mixing probabilities. Pearson focuses on the case of two gaussians which he believes is of special importance in the context of evolutionary biology:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A family probably breaks up first into two species, rather than three or more, owing to the pressure at a given time of some particular form of natural selection.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;With this reasoning he stipulates that the crab measurements are drawn from a mixture of two gaussians and aims to recover the parameters of this mixture. Learning a mixture of two gaussians at the time was a formidable task that lead Pearson to come up with a powerful approach. His approach is based on the method of moments which uses the empirical moments of a distribution to distinguish between competing models. Given \(n\) samples \(x_1,...,x_n\) the \(k\)-th empirical moment is defined as \(\frac1n\sum_ix_i^k\), which for sufficiently large \({n}\) will approximate the true moment \({\mathbb{E}\,x_i^k}\). A mixture of two one-dimensional gaussians has \({5}\) parameters so one might hope that \({5}\) moments are sufficient to identify the parameters.&lt;/p&gt;
&lt;p&gt;Pearson derived a ninth degree polynomial \({p_5}\) in the first \({5}\) moments and located the real roots of this polynomial over a carefully chosen interval. Each root gives a candidate mixture that matches the first \({5}\) moments; there were two valid solutions, among which Pearson selected the one whose \({6}\)-th moment was closest to the observed empirical \({6}\)-th moment.&lt;/p&gt;
&lt;p&gt;Pearson goes on to evaluate this method numerically on the crab samples---by hand. In doing so he touches on a number of issues such as &lt;em&gt;numerical stability&lt;/em&gt; and &lt;em&gt;number of floating point operations&lt;/em&gt; that seem to me as being well ahead of their time. Pearson was clearly aware of computational constraints and optimized his algorithm to be computationally tractable.&lt;/p&gt;
&lt;h2&gt;Learning mixtures of gaussians&lt;/h2&gt;
&lt;p&gt;Pearson&#39;s seminal paper proposes a problem and a method, but it doesn&#39;t give an analysis of the method. Does the method really work on &lt;em&gt;all&lt;/em&gt; instances or did he get lucky on the crab population? Is there &lt;em&gt;any&lt;/em&gt; efficient method that works on all instances? The sort of theorem we would like to have is: Given \({n}\) samples from a mixture of two gaussians, we can estimate each parameter up to small additive distance in polynomial time.&lt;/p&gt;
&lt;p&gt;Eric Price and I have a &lt;a href=&quot;http://arxiv.org/abs/1404.4997&quot;&gt;recent result&lt;/a&gt; that resolves this problem by giving a computationally efficient estimator that achieves an optimal bound on the number of samples:&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Denoting by \({\sigma^2}\) the overall variance of the mixture, \({\Theta(\sigma^{12})}\) samples are necessary and sufficient to estimate each parameter to constant additive distance. The estimator is computationally efficient and extends to the \({d}\)-dimensional mixture problem up to a necessary factor of \({O(\log d)}\) in the sample requirement.&lt;/p&gt;
&lt;p&gt;Strikingly, our one-dimensional estimator turns out to be very closely related to Pearson&#39;s approach. Some extensions are needed, but I&#39;m inclined to say that our result can be interpreted as showing that Pearson&#39;s original estimator is in fact an optimal solution to the problem he proposed.&lt;/p&gt;

&lt;p&gt;Until a recent breakthrough result by &lt;a href=&quot;http://people.csail.mit.edu/moitra/docs/2g-full.pdf&quot;&gt;Kalai, Moitra and Valiant&lt;/a&gt; (2010), no polynomial bounds for the problem were known except under stronger assumptions on the gaussian components.&amp;lt;&lt;/p&gt;

&lt;h3&gt; A word about definitions&lt;/h3&gt;
&lt;p&gt;Of course, we need to be a bit more careful with the above statements. Clearly we can only hope to recover the parameters up to permutation of the two gaussian components as any permutation gives an identical mixture. Second, we cannot hope to learn the mixture probabilities in general up to small error. If, for example, the two gaussian components are indistinguishable, there is no way of learning the mixture probabilities---even though we can estimate means and variances easily by treating the distribution as one gaussian. On the other hand, if the gaussian components are distinguishable then it is not hard to estimate the mixture probabilities from the other parameters and the overall mean and variance of the mixture. For this reason it makes some sense to treat the mixture probabilities separately and focus on learning means and variances of the mixture.&lt;/p&gt;
&lt;p&gt;Another point to keep in mind is that our notion of parameter distance should be scale-free. A reasonable goal is therefore to learn parameters \({\hat \mu_1,\hat \mu_2,\hat\sigma_1,\hat\sigma_2}\) such that if \({\mu_1,\mu_2,\sigma_1,\sigma_2}\) are the unknown parameters, there is a permutation \({\pi\colon\{1,2\}\rightarrow\{1,2\}}\) such that for all \({i\in\{1,2\},}\)&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;\(\displaystyle \max\left( \big|\mu_i-\mu_{\pi(i)}\big|^2, \big|\sigma_i^2-\sigma_{\pi(i)}^2\big|\right)\le\epsilon \cdot \sigma^2. \)&lt;/p&gt;
&lt;p&gt;Here, \({\sigma^2}\) is the overall variance of the mixture. The definition extends to the \({d}\)-dimensional problem for suitable notion of variance (essentially the maximum variance in each coordinate) and by replacing absolute values with \({\ell_\infty}\)-norms.&lt;/p&gt;
&lt;h2&gt;Some intuition for the proof&lt;/h2&gt;
&lt;p&gt;While the main difficulty in the proof is the upper bound, let&#39;s start with some intuition for why we can&#39;t hope to do better than \({\sigma^{12}}\). Just like Pearson&#39;s estimator and that of Kalai, Moitra and Valiant, our estimator is based on the first six moments of the distribution. It is not hard to show that estimating the \({k}\)-th moment up to error \({\epsilon\sigma^k}\) requires \({\Omega(1/\epsilon^2)}\) samples. Hence, learning each of the first six moments to constant error needs \({\Omega(\sigma^{12})}\) samples. This doesn&#39;t directly imply a lower bound for the mixture problem. After all, learning mixtures could be strictly easier than estimating the first six moments. Nevertheless, we know from Pearson that there are two mixtures that have matching first \({5}\) moments. This turns out to give you a huge hint as to how to prove a lower bound for the mixture problem. The key observation is that two mixtures that have matching first \({5}\) moments and constant parameters are extremely close to each other after adding a gaussian variable \({N(0,\sigma^2)}\) to both mixtures for large enought \({\sigma^2}\). Note that we still have two mixtures of gaussians after this operation and the total variance is \({O(\sigma^2).}\)&lt;/p&gt;
&lt;p&gt;Making this statement formal requires choosing the right probability metric. In our case this turns out to be the squared Hellinger distance. Sloppily identifying distributions with density functions like only a true computer scientist would do, the squared Hellinger distance is defined as&lt;/p&gt;

&lt;p&gt;\[\mathrm{H}^2(f,g) = \frac12\int_{-\infty}^{\infty}
\Big(\sqrt{f(x)}-\sqrt{g(x)}\Big)^2 \mathrm{d}x\,. \]&lt;/p&gt;

&lt;p style=&quot;padding-left: 30px;&quot;&gt;&lt;strong&gt;Lemma.&lt;/strong&gt; Let \({f,g}\) be mixtures with matching first \({k}\) moments and constant parameters. Let \({p,q}\) be the distributions obtained by adding \({N(0,\sigma^2)}\) to each distribution. Then, \({\mathrm{H}^2(p,q)\le O(1/\sigma^{2k+2})}\).&lt;/p&gt;
&lt;p&gt;The lemma immediately gives the lower bound we need. Indeed, the squared Hellinger distance is sub-additive on product distributions. So, it&#39;s easy to see what happens for \({n}\) independent samples from each distribution. This can increase the squared Hellinger distance by at most a factor \({n}\). In particular, for any \({n=o(\sigma^{2k+2})}\) the squared Hellinger distance between \({n}\) samples from each distribution is at most \({o(1)}\). Owing to a useful relation between the Hellinger distance and total variation, this implies that also the total variation distance is at most \({o(1)}\). Using the definition of total variation distance, this means no estimator (efficient or not) can tell apart the two distributions from \({n=o(\sigma^{2k+2})}\) samples with constant success probability. In particular, parameter estimation is impossible.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/moo32.png&quot; alt=&quot;Mixtures with identical first 5 moments&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This lemma is an example of a broader phenomenon: Matching moments plus &quot;noise&quot; implies tiny Hellinger distance. A great reference is Pollard&#39;s &lt;a href=&quot;http://books.google.com/books/about/A_User_s_Guide_to_Measure_Theoretic_Prob.html?id=B7Ch-c2G21MC&quot;&gt;book&lt;/a&gt; and &lt;a href=&quot;http://www.stat.yale.edu/~pollard/Books/Asymptopia/Metrics.pdf&quot;&gt;lecture notes&lt;/a&gt;. By the way, Pollard&#39;s expository writing is generally wonderful. For instance, check out his (unrelated) paper on &lt;a href=&quot;http://www.stat.yale.edu/~jtc5/papers/ConditioningAsDisintegration.pdf&quot;&gt;&lt;em&gt;guilt-free manipulation of conditional densities&lt;/em&gt;&lt;/a&gt; if you ever wondered about how to condition on a measure zero event without regret. Not that computer scientists are generally concerned about it.&lt;/p&gt;
&lt;h3&gt;Matching upper bound&lt;/h3&gt;
&lt;p&gt;The matching upper bound is a bit trickier. The proof uses a convenient notion of &lt;em&gt;excess moments&lt;/em&gt; that was introduced by Pearson. You might have heard of excess kurtosis (the fourth excess moment) as it appears in the info box on every Wikipedia article about distributions. Excess moments have the appealing property that they are invariant under adding and subtracting a common term from the variance of each component.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/moo4.png&quot; alt=&quot;Identical excess moments&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The second excess moment is always \({0.}\) As the picture suggests, we can make one component increasingly spiky and think of it as a distribution that places all its weight on a single point. In accordance with this notion of excess moments it makes sense to reparametrize the mixture in such a way that the parameters are also independent of adding and subtracting a common variance term. Assuming the overall mean of the mixture is \({0,}\) this leaves us with three free parameters \({\alpha,\beta,\gamma.}\) The third through sixth excess moment give us four equations in these three parameters.&lt;/p&gt;
&lt;p&gt;Expressing the excess moments in terms of our new parameters \({\alpha,\beta,\gamma,}\) we can derive in a fairly natural manner a ninth degree polynomial \({p_5(y)}\) whose coefficients depend on the first \({5}\) excess moments so that \({\alpha}\) has to satisfy \({p_5(\alpha)=0.}\) The polynomial \({p_5}\) was already used by Pearson. Unfortunately, \({p_5}\) can have multiple roots and this is to be expected since \({5}\) moments are not sufficient to identify a mixture of two gaussians. Pearson computed the mixtures associated with each of the roots and threw out the invalid solutions (e.g. the ones that give imaginary variances), getting two valid mixtures that matched on the first five moments. He then chose the one whose sixth moment was closest to the observed sixth moment.&lt;/p&gt;
&lt;p&gt;We proceed somewhat differently from Pearson after computing \({p_5}\). We derive another polynomial \({p_6}\) (depending on all six moments) and a bound \({B}\) such that \({\alpha}\) is the only solution to the system of equations&lt;/p&gt;

&lt;p&gt;\[ \big\{p_5(y)=0,\quad p_6(y)=0,\quad 0&amp;lt; y\leq B\big\}. \]&lt;/p&gt;

&lt;p&gt;This approach isn&#39;t yet robust to small perturbations in the moments; for example, if \({p_5}\) has a double root at \({\alpha}\), it may have no nearby root after perturbation. We therefore consider the polynomial \({r(y) = p_5^2(y)+p_6^2(y)}\) which we know is zero at \({\alpha.}\) We argue that \({r(y)}\) is significantly nonzero for any \({y}\) significantly far from \({\alpha}\). This is the main difficulty in the proof, but if you really read this far, it&#39;s perhaps not a bad point to switch to reading &lt;a href=&quot;http://arxiv.org/abs/1404.4997&quot;&gt;our paper.&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;I sure drooled enough over how much I like Pearson&#39;s paper. Let me conclude with a different practical thought. There were a bunch of tools (as in actual computer programs---&lt;em&gt;gasp!&lt;/em&gt;) that made the work a lot easier. Before we proved the lower bound we had verified it using arbitrary floating point precision numerical integration to accuracy \({10^{-300}}\) for \({\sigma^2=10,100,1000,\dots}\) and the decrease in Hellinger distance was exactly what it should be. We used a Python package called &lt;a href=&quot;https://code.google.com/p/mpmath/&quot;&gt;mpmath&lt;/a&gt; for that.&lt;/p&gt;
&lt;p&gt;Similarly, for the upper bound we used a symbolic computation package in Python called &lt;a href=&quot;http://sympy.org/en/index.html&quot;&gt;SymPy&lt;/a&gt; to factor various polynomials, perform substitutions and multiplications. It would&#39;ve been tedious, time-consuming and error prone to do everything by hand (even if the final result is not too hard to check by hand).&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;&lt;em&gt;To stay on top of future posts, subscribe
to the &lt;a style=&quot;color: #bc360a;&quot; href=&quot;http://blog.mrtz.org/feed.xml&quot;&gt;RSS feed&lt;/a&gt; or follow me on &lt;a style=&quot;color: #bc360a;&quot; href=&quot;http://twitter.com/mrtz&quot;&gt;Twitter&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 21 Apr 2014 18:02:37 -0700</pubDate>
        <link>http://blog.mrtz.org/2014/04/21/pearsons-polynomial.html</link>
        <guid isPermaLink="true">http://blog.mrtz.org/2014/04/21/pearsons-polynomial.html</guid>
        
        <category>algorithms</category>
        
        <category>simons</category>
        
        <category>tcs</category>
        
        <category>theory</category>
        
        <category>polynomials</category>
        
        <category>statistics</category>
        
        <category>mixtures</category>
        
        <category>gaussian</category>
        
        <category>learning</category>
        
        
      </item>
    
      <item>
        <title>False Discovery and Differential Privacy</title>
        <description>&lt;p&gt;The Simons program on Big Data wrapped up about a month ago with a workshop on &lt;a href=&quot;http://simons.berkeley.edu/workshops/bigdata2013-4&quot;&gt;Big Data and Differential Privacy&lt;/a&gt;. With the possible exception of one talk on the last day after lunch, it was a really fabulous workshop. You would&#39;ve enjoyed attending even if you were neither interested in Big Data nor Differential Privacy. One reason is that the organizers aimed to address a problem that transcends both machine learning and privacy, a problem relevant to all empirical sciences. The problem is &lt;strong&gt;false discovery&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If you&#39;ve never seen it, you should check out this XKCD comic explaining why &lt;a href=&quot;http://imgs.xkcd.com/comics/significant.png&quot;&gt;green jelly beans cause acne&lt;/a&gt;. The idea is that if you evaluate enough hypotheses on the same data set, eventually you will find something significant. The availability of public data sets used by thousands of scientists greatly exacerbates the problem. An important example is the area of &lt;a href=&quot;http://en.wikipedia.org/wiki/Genome-wide_association_study&quot;&gt;Genome Wide Association Studies&lt;/a&gt; (GWAS).&lt;/p&gt;
&lt;p&gt;A typical study has a few hundred or perhaps a few thousands patient DNA sequences. Each sequence contains a few hundred thousands so-called &lt;a href=&quot;http://en.wikipedia.org/wiki/Single-nucleotide_polymorphism&quot;&gt;SNPs&lt;/a&gt;. Scientists check each SNP for possible correlation with a particular phenotype. Significance tests and p-values are part of the standard method. But what p-value is reasonable? In GWAS typical p-values are as small as \({10^{-6}}\) or even \({10^{-7}.}\) They get smaller ever year as the SNP resolution of the sequenced genome increases. How many SNPs are incorrectly declared significant? How do we interpret these scientific findings?&lt;/p&gt;
&lt;p&gt;A similar discussion arose recently in the FDA induced &lt;a href=&quot;http://www.bioedge.org/index.php/bioethics/bioethics_article/10799&quot;&gt;shutdown of 23andMe&lt;/a&gt;. Lior Pachter started a &lt;a href=&quot;http://liorpachter.wordpress.com/2013/11/30/23andme-genotypes-are-all-wrong/&quot;&gt;heated discussion&lt;/a&gt; on his blog about why the findings of of 23andMe are unreliable. In short: Too many hypotheses evaluated against a single genome. Meanwhile, Scott Aaronson asserts his right to &lt;a href=&quot;http://www.scottaaronson.com/blog/?p=1615&quot;&gt;misinterpret probabilities&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;GWAS is certainly not the only example. Some think a major &lt;a href=&quot;http://www.economist.com/news/briefing/21588057-scientists-think-science-self-correcting-alarming-degree-it-not-trouble&quot;&gt;scientific crisis is upon us.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, what can we do about it? A person who has spent at least two decades thinking about this problem is Yoav Benjamini. Luckily, he agreed to give a tutorial about his work at the privacy workshop revolving around the idea of &lt;a href=&quot;http://en.wikipedia.org/wiki/False_discovery_rate&quot;&gt;False Discovery Rate&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;False Discovery Rate&lt;/h1&gt;
&lt;p&gt;The basic setup is this. There are \({m}\) null hypotheses \({h_1,\dots,h_m}\) that we would like to test. A discovery corresponds to a rejected null hypothesis. Let \({V}\) be the random variable counting the number of false discoveries, i.e., rejected null hypotheses that are actually true. We also let \({R}\) denote the total number of rejected hypothesis, i.e., all discoveries both true and false. The false discovery rate (FDR) is the expectation of the ratio \({V/R,}\) where we define the ratio as \({0}\) if \({R=0.}\)&lt;/p&gt;
&lt;p&gt;The idea behind this definition is that if there are many discoveries, it might be tolerable to make some false discoveries. However, if all null hypotheses are true, all discoveries are false. So, even a single discovery brings the FDR up to \({1}\) (the largest possible value).&lt;/p&gt;
&lt;p&gt;I encourage you to check out Omer Reingold&#39;s &lt;a href=&quot;http://simons.berkeley.edu/talks/omer-reingold-2013-12-11&quot;&gt;Musings on False Discovery Rate&lt;/a&gt; from a Computer Scientist&#39;s point of view.&lt;/p&gt;
&lt;p&gt;Now, suppose we want to design an experiment while controlling the FDR. That is we want to make sure that the FDR is at most some \({\alpha&amp;lt; 1.}\) Benjamini and Hochberg suggested an algorithm for doing exactly that. With more than 20000 citations this might be one of the most widely cited algorithms:&lt;/p&gt;
&lt;p&gt;Let \({p_1,\dots,p_m}\) be \({p}\)-values corresponding to our \({m}\) hypotheses. Sort these \({p}\)-values in increasing order \({p_{(1)}\le \dots\le p_{(m)}.}\)&lt;/p&gt;
&lt;p&gt;1. Find the largest \({k}\) such that \({p_{(k)} \le \frac{k}{m}\cdot \alpha}\).&lt;/p&gt;
&lt;p&gt;2. Reject \({h_{(1)},\dots,h_{(k)}.}\)&lt;/p&gt;
&lt;p&gt;An important theorem they proved is that indeed this procedure controls the false discovery rate under certain assumptions. For example, if our test statistics are independent of each other, this holds. Of course, independence is a very strong assumption and a lot of work in statistics aims to weaken these assumptions.&lt;/p&gt;
&lt;p&gt;My general feeling about this definition is that it is optimistic in several regards. The first is that we only talk about expectations. It is easy to come up with examples where the expectation is small but the with small but non-negligible probability there are many false discoveries. Second, we need several fairly strong assumptions to be able to prove that such a procedure actually controls the false discovery rate. Finally, the method puts lot of faith in careful experimental design and proper execution. In particular to apply the methodology we need knowledge of what hypotheses we might test.&lt;/p&gt;
&lt;p&gt;Nevertheless, this optimism in the definition leads to usable and realistic answers as to how large the sample should be in concrete experimental setups. This must have been a major factor in the success of this methodology.&lt;/p&gt;
&lt;h1&gt;Differential Privacy&lt;/h1&gt;
&lt;p&gt;How is any of this related to privacy? The short answer is that Differential Privacy by itself controls false discovery in a certain precise sense---though formally different from the above. Indeed, I will formally show below that Differential Privacy implies small &lt;a href=&quot;http://en.wikipedia.org/wiki/Generalization_error&quot;&gt;&quot;generalization error&quot;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is by no means an accident of the definition. There is a deeper reason for it. Intuitively, Differential Privacy ensures that the outcome of a statistical analysis does not depend on the specifics of the data set but rather the underlying properties of the population. This is why it gives privacy. I may be able to learn from the data that smoking causes cancer, but I wouldn&#39;t be able to learn that a particular individual in the database, say Donald Draper, smokes and has cancer. The flip side is that Differential Privacy does not allow you to access the data set arbitrarily often. Instead it attaches a cost to each operation and an overall budget that you better not exceed. The most common complain about Differential Privacy is that it doesn&#39;t allow you to do whatever the heck you want with the data. My response is usually that neither does statistics. A data set has limited explanatory power. Use it carelessly and you&#39;ll end up with false discovery. I think it&#39;s a feature of Differential Privacy---not a shortcoming---that it makes this fundamental fact of nature explicit.&lt;/p&gt;
&lt;p&gt;If this philosophical excursion struck you as unconvincing, let me try the formal route. This argument is folklore by the way. I learned it from Guy Rothblum a few years ago who attributed it to Frank McSherry at the time. Let me know if I&#39;m missing the right attribution.&lt;/p&gt;
&lt;p&gt;Suppose we want to learn a concept \({c\colon\{0,1\}^d \rightarrow \{0,1\}}\) from labeled examples \({D=\{(x_1,l_1),\dots,(x_m,l_m)\}}\) drawn from some underlying distribution \({P}\).&lt;/p&gt;
&lt;p&gt;Now, suppose we use an \({\epsilon}\)-differentially private learning algorithm \(\) for the task. That is \(\) is a randomized algorithm such that changing one example has little effect on the output distribution of the algorithm. Formally, for any set of examples \({D&#39;}\) that differs from \({D}\) in only one pair, we have for any set of output hypotheses \({H}\):&lt;/p&gt;

&lt;p&gt;\[\Pr \{ A(D) \in H \} \ge e^{-\epsilon}\Pr\{ A(D’) \in H \}. \]&lt;/p&gt;

&lt;p&gt;Furthermore, assume that \(\) is \({\alpha}\)-useful in the sense that it always outputs a hypothesis \({h}\) that has error at most \({\alpha}\) on the training examples. Here, \({\epsilon,\alpha \ll 1.}\) (By the way, I&#39;m using the word &quot;hypothesis&quot; here in its learning theory sense not the &quot;null hypothesis&quot; sense.)&lt;/p&gt;
&lt;p&gt;I claim that with the output of \(\) must have error at most \({O(\epsilon + \alpha)}\) on the underlying distribution \({P.}\) This means precisely that the output generalizes. Importantly, we didn&#39;t assume anything about the hypothesis class! We only assume that the learner is differentially private.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof sketch:&lt;/strong&gt; Pick a fresh example \({(x^*,l^*)}\) from the distribution \({P.}\) Let \({H^*= \{ h \colon h(x^*) = c(x^*) \}}\) be the set of hypotheses that agree with the concept \({c}\) on this example \({x^*.}\) Let \({D&#39;}\) be the example set where we remove a random element from \({D}\) and replace it with \({(x^*,l^*).}\) Since, \(\) is useful, it must be the case that \({h&#39; = {\cal A}(D&#39;)}\) has error at most \({\alpha}\) on \({D&#39;}\). In particular, it classifies \({x^*}\) correctly with probability \({1-\alpha.}\) This is because all examples in \({D&#39;}\) are identically distributed. Formally, \({\Pr\{ {\cal A}(D&#39;) \in H^* \}\ge 1-\alpha.}\) Note the randomness is now over both \(\) and the replacement we did. Appealing to the definition of Differential Privacy above, it follows that&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;\(\displaystyle \Pr\{ A(D) \in H^*\} \ge e^{-\epsilon}(1-\alpha) \ge 1 - O(\epsilon + \alpha). \)&lt;/p&gt;
&lt;p&gt;That means \({A(D)}\) is correct on an *unseen* example from \({D}\) with high probability.&lt;/p&gt;
&lt;p&gt;The above is just a simple example of a broader phenomenon showing that &lt;a href=&quot;http://jmlr.org/papers/v2/bousquet02a.html&quot;&gt;stability of a learning algorithm implies generalization bounds&lt;/a&gt;. Differential Privacy is a strong form of stability that implies most notions of stability studied in learning theory.&lt;/p&gt;
&lt;h1&gt;What&#39;s next?&lt;/h1&gt;
&lt;p&gt;False discovery and privacy loss are two persistent issues we&#39;re facing in the era of data analysis. The two problems share some fundamental characteristics. It could be very fruitful think about both problems in relation to each other, or, even as &lt;em&gt;facets of the same underlying problem&lt;/em&gt;. Technically, there seems to be some unexplored middle ground between FDR and Differential Privacy that I&#39;d like to understand better.&lt;/p&gt;
&lt;p&gt;The discussion here is also closely related to my first post of the semester in which I asked: &lt;a href=&quot;http://mrtz.org/blog/what-should-a-theory-of-big-data-do/&quot;&gt;What should a theory of big data do?&lt;/a&gt; I argued that besides posing concrete technical challenges, big data poses a deep conceptual problem. We seem to have difficulty capturing the way people interact with data today. This makes it hard for us to get control over the things that can go wrong. I seem to have come a full circle asking the same question again. Of course, I knew that this was too broad a question to be resolved in the span of four months.&lt;/p&gt;
</description>
        <pubDate>Mon, 13 Jan 2014 13:05:52 -0800</pubDate>
        <link>http://blog.mrtz.org/2014/01/13/false-discovery.html</link>
        <guid isPermaLink="true">http://blog.mrtz.org/2014/01/13/false-discovery.html</guid>
        
        <category>simons</category>
        
        <category>big data</category>
        
        <category>tcs</category>
        
        <category>privacy</category>
        
        <category>false discovery</category>
        
        
      </item>
    
      <item>
        <title>Power Method still Powerful </title>
        <description>&lt;p&gt;The Power Method is the oldest practical method for finding the eigenvector of a matrix corresponding to the eigenvalue of largest modulus. Most of us will live to celebrate the Power Method&#39;s 100th birthday around 2029. I plan to throw a party. What&#39;s surprising is that the Power Method continues to be highly relevant in a number of recent applications in algorithms and machine learning. I see primarily two reasons for this. One is computational efficiency. The Power Method happens to be  highly efficient both in terms of memory footprint and running time. A few iterations often suffice to find a good approximate solution. The second less appreciated but no less important aspect is &lt;em&gt;robustness&lt;/em&gt;. The Power Method is quite well-behaved in the presence of noise making it an ideal candidate for a number of applications. It&#39;s also easy to modify and to extend it without breaking it. Robust algorithms live longer. The Power Method is a great example. We saw another good one before. (Hint: &lt;a href=&quot;http://mrtz.org/blog/the-zen-of-gradient-descent/&quot;&gt;Gradient Descent&lt;/a&gt;) In this post I&#39;ll describe a very useful robust analysis of the Power Method that is not as widely known as it sould be. It characterizes the convergence behavior of the algorithm in a strong noise model and comes with a neat geometric intuition.&lt;/p&gt;
&lt;h2&gt;Power Method: Basics&lt;/h2&gt;
&lt;p&gt;Let&#39;s start from scratch. The problem that the power method solves is this:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;Given \({A,}\) find the rank \({1}\) matrix \({B}\) that minimizes \({\|A-B\|_F^2.}\)&lt;/p&gt;
&lt;p&gt;Here, \({\|A-B\|_F^2}\) is the squared Frobenius norm of the difference between \({A}\) and \({B.}\) (Recall that this is the sum of squared entries of \(A-B\)). Any rank \({1}\) matrix can of course be written as \({xy^T}\) (allowing for rectangular \({A}\)). On the face of it, this is a non-convex optimization problem and there is no a priori reason that it should be easy! Still, if we think of \({f(x,y) = \|A-xy^T\|_F^2}\) as our objective function, we can check that setting the gradient of \({f}\) with respect to \({x}\) to zero gives us the equation by \({x=Ay}\) provided that we normalized \({y}\) to be a unit vector. Likewise taking the gradient of \({f}\) with respect to \({y}\) after normalizing \({x}\) to have unit norm leads to the equation \(y={A^T x.}\) The Power Method is the simple algorithm which repeatedly performs these update steps. As an aside, this idea of solving a non-convex low-rank optimization problem via alternating minimization steps is a powerful heuristic for a number of related problems.&lt;/p&gt;
&lt;p&gt;For simplicity from here on we&#39;ll assume that \({A}\) is symmetric. In this case there is only one update rule: \({x_{t+1} = Ax_{t}.}\) It is also prudent to normalize \({x_t}\) after each step by dividing the vector by its norm.&lt;/p&gt;
&lt;p&gt;The same derivation of the Power Method applies to the more general problem: Given a symmetric \({n\times n}\) matrix \({A,}\) find the rank \({k}\) matrix \({B}\) that minimizes \({\|A-B\|_F^2}\). Note that the optimal rank \({k}\) matrix is also given by the a truncated singular value decomposition of \({A.}\) Hence, the problem is equivalent to finding the first \({k}\) singular vectors of \({A.}\)&lt;/p&gt;
&lt;p&gt;A symmetric rank \({k}\) matrix can be written as \({B= XX^T}\) where \({X}\) is an \({n\times k}\) matrix. The update rule is therefore \({X_{t+1} = AX_{t}}\). There are multiple ways to normalize \({X_t}\). The one we choose here is to ensure that \({X_t}\) has orthonormal columns. That is each column has unit norm and is orthogonal to the other columns. This is can be done using good ol&#39; boys  &lt;a title=&quot;Gram Schmidt&quot; href=&quot;http://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process&quot;&gt;Gram-Schmidt&lt;/a&gt; (though in practice people prefer the Householder method).&lt;/p&gt;
&lt;p&gt;The resulting algorithm is often called Subspace Iteration. I like this name. It stresses the point that the only thing that will matter to us is the subspace spanned by the columns of \({X_t}\) and not the particular basis that the algorithm happens to produce. This viewpoint is important in the analysis of the algorithm that we&#39;ll see next.&lt;/p&gt;
&lt;h2&gt;A Robust Geometric Analysis of Subspace Iteration&lt;/h2&gt;
&lt;p&gt;Most of us know one analysis of the power method. You write your vector \({x_t}\) in the eigenbasis of \({A}\) and keep track of how matrix vector multiplication manipulates the coefficients of the vector in this basis. It works and it&#39;s simple, but it has a couple of issues. First, it doesn&#39;t generalize easily to the case of Subspace Iteration. Second, it becomes messy in the presence of errors. So what is a better way? What I&#39;m going to describe was possibly done by numerical analysts in the sixties before the community moved on to analyze more modern eigenvalue methods.&lt;/p&gt;
&lt;h3&gt;Principal Angles&lt;/h3&gt;
&lt;p&gt;To understand what happens in Subspace Iteration at each step, it&#39;ll be helpful to have a potential function and to understand under which circumstances it decreases. As argued before the potential function should be basis free to avoid syntactic arguments. The tool we&#39;re going to use is the &lt;strong&gt;principal angle&lt;/strong&gt; between subspaces. Before we see a formal definition, it turns out that much of what&#39;s true for the angle between two vectors can be lifted to the case of subspaces of equal dimension. In fact, much of what&#39;s true for the angle between spaces of equal dimension generalizes to unequal dimensions with some extra care.&lt;/p&gt;
&lt;p&gt;The two subspaces we want to compare are the space spanned by the \({k}\) columns of \({X_t}\) and the space \({U}\) spanned by the \({r}\) dominant singular vectors of \({A.}\) For now, we&#39;ll discuss the case where \({r=k.}\) I&#39;ll be a bit sloppy in my notation and use the letters \({U,X_t}\) for these subspaces.&lt;/p&gt;
&lt;p&gt;Let&#39;s start with \({k=1}\). Here the cosine of the angle \({\theta}\) between the two unit vectors \({X_t}\) and \({U}\) is of course defined as \({\cos\theta(U,X_t)= |U^T X_t|.}\) It turns out that the natural generalization for \({k&amp;gt;1}\) is to define&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle \cos\theta(U,X_t)=\sigma_{\mathrm{min}}(U^T X_t). \)&lt;/p&gt;
&lt;p&gt;In words, the cosine of the angle between \({U}\) and \({X_t}\) is the smallest singular value of the \({k\times k}\) matrix \({U^T X_t}\). Here&#39;s a sanity check on our definition. If the smallest singular value is \({0}\) then there is a nonzero vector in the range of the matrix \({X_t}\) that is orthogonal to the range of \({U}\). In an intuitive sense this shows that the subspaces are quite dissimilar. To get some more intuition, consider a basis \({V}\) for the orthogonal complement of \({U.}\) It makes sense to define \({\sin\theta(U,X_t)= \sigma_{\mathrm{max}}(V^T X_t)}\). Indeed, we can convince ourselves that this satisfies the familiar rule \({1= \sin^2\theta + \cos^2\theta}\). Finally, we define \({\tan\theta}\) as the ratio of sine to cosine.&lt;/p&gt;
&lt;h3&gt;A strong noise model&lt;/h3&gt;
&lt;p&gt;Let&#39;s be ambitious and analyze Subspace Iteration in a strong noise model. It turns out that this will not actually make the analysis a lot harder, but a lot more general. The noise model I like is one in which each matrix-vector product is followed by the addition of a possibly adversarial noise term. Specifically, the only way we can access the matrix \(A\) is through an operation of the form&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle Y = AX + G. \)&lt;/p&gt;
&lt;p&gt;Here, \({X}\) is what we get to choose and \({G}\) is the noise matrix we may not get to choose. We assume that \(G$ is the only source of error and that arithmetic is exact. In this model our algorithm proceeds as follows:&lt;/p&gt;

&lt;div style=&quot;margin:10px;border: 1px solid #bbb; padding: 10px;&quot;&gt;Pick \({X_0.}\) For \({t = 1}\) to \({t=L:}\)
&lt;ol&gt;
&lt;li&gt;\({Y_t = AX_{t-1} + G_t}\)&lt;/li&gt;
&lt;li&gt;\({X_t = \text{Orthonormalize}(Y_t)}\)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

&lt;h3&gt;The main geometric lemma&lt;/h3&gt;
&lt;p&gt;To analyze the above algorithm, we will consider the potential function \({\tan\theta(U,X_t).}\) If we pick \({X_0}\) randomly it&#39;s not so hard to show that it starts out being something like \({O(\sqrt{kn})}\) with high probability. We&#39;ll argue that \({\tan\theta(U,X_t)}\) decreases geometrically at the rate of \({\sigma_{k+1}/\sigma_{k}}\) under some assumptions on \({G_t}\). Here, \({\sigma_k}\) is the \({k}\)-th largest singular value. The analysis therefore requires a separation between \({\sigma_k}\) and \({\sigma_{k+1}}\). (NB: We can get around that by taking the dimension \({r}\) of \({U}\) to be larger than \({k.}\) In that case we&#39;d get a convergence rate of the form \({\sigma_{r+1}/\sigma_k.}\) The argument is a bit more complicated, but can be found &lt;a title=&quot;Robust Subspace Iteration and Privacy-Preserving Spectral Analysis&quot; href=&quot;http://arxiv.org/abs/1311.2495&quot;&gt;here&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Now the lemma we&#39;ll prove is this:&lt;/p&gt;
&lt;p style=&quot;text-align: left;&quot;&gt;&lt;strong&gt;Lemma.&lt;/strong&gt; For every \({t\ge 1,}\)&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle \tan\theta(U,X_t) \le \frac{\sigma_{k+1}\sin\theta(U,X_{t-1})+\|V^T G_t\|} {\sigma_k\cos\theta(U,X_{t-1})-\|U^T G_t\|}. \)&lt;/p&gt;
&lt;p&gt;This lemma is quite neat. First of all if \({G_t=0,}\) then we recover the noise-free convergence rate of Subspace Iteration that I claimed above. Second what this lemma tells us is that the sine of the angle between \({U}\) and \({X_{t-1}}\) gets perturbed by the norm of the projection of \({G_t}\) onto \({V}\), the orthogonal complement of \({U.}\) This should not surprise us, since \({\sin\theta(U,X_{t-1})=\|V^T X_{t-1}\|.}\) Similarly, the cosine of the angle is only perturbed by the projection of \({G_t}\) onto \({U.}\) This again seems intuitive given the definition of the cosine. An important consequence is this. Initially, the cosine between \({U}\) and \({X_0}\) might be very small, e.g. \({\Omega(\sqrt{k/n})}\) if we pick \({X_0}\) randomly. Fortunately, the cosine is only affected by a \({k}\)-dimensional projection of \({G_t}\) which we expect to be much smaller than the total norm of \({G_t.}\)&lt;/p&gt;
&lt;p&gt;The lemma has an intuitive geometric interpretation expressed in the following picture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/power-method.png&quot; alt=&quot;Power method&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It&#39;s straightforward to prove the lemma when \({k=1}\). We simply write out the definition on the left hand side, plug in the update rule that generated \({X_t}\) and observe that the normalization terms in the numerator and denominator cancel. This leaves us with the stated expression after some simple manipulations. For larger dimension we might be troubled by the fact that we applied the Gram-Schmidt algorithm. Why would that cancel out? Interestingly it does. Here&#39;s why. The tangent satisfies the identity:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle \tan\theta(U,X_t) = \|(V^T X_t)(U^TX_t)^{-1}\|. \)&lt;/p&gt;
&lt;p&gt;It&#39;s perhaps not obvious, but not more than an exercise to check this. On the other hand, \({X_t = Y_t R}\) for some invertible linear transformation \({R.}\) This is a property of orthonormalization. Namely, \({X_t}\) and \({Y_t}\) must have the same range. But \({(U^TY_tR)^{-1}=R^{-1}(U^T Y_t)^{-1}}\). Hence&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle \|(V^T X_t)(U^TX_t)^{-1}\| = \|(V^T Y_t)(U^TY_t)^{-1}\|. \)&lt;/p&gt;
&lt;p&gt;It&#39;s now easy to proceed. First, by the submultiplicativity of the spectral norm,&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle \|(V^T Y_t)(U^TY_t)^{-1}\| \le \|(V^T Y_t)\|\cdot\|(U^TY_t)^{-1}\|. \)&lt;/p&gt;
&lt;p&gt;Consider the first term on the right hand side. We can bound it as follows:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle \|(V^T Y_t)\|\!\le\!\|V^TAX_{t-1}\| + \|V^T G_t\|\!\le\! \sigma_{k+1}\sin\theta(U,X_{t-1}) + \|V^T G_t\|. \)&lt;/p&gt;
&lt;p&gt;Here we used that the operation \({V^T A}\) eliminates the first \({k}\) singular vectors of \({A.}\) The resulting matrix has spectral norm at most \({\sigma_{k+1}}\).&lt;/p&gt;
&lt;p&gt;The second term follows similarly. We have \({\|(U^T Y_t)^{-1}\| = 1/\sigma_{\mathrm{min}}(U^TY_t)}\) and&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;\(\displaystyle\!\!\!\sigma_{\mathrm{min}}(U^TY_{t})\!\ge\!\sigma_{\mathrm{min}}(U^TAX_{t-1})\!-\!\|U^T G_t\|\!\ge\! \sigma_k\cos\theta(U,X_{t-1})\!-\!\|U^T G_t\|\!\!\!\)&lt;/p&gt;
&lt;p&gt;This concludes the proof of the lemma.&lt;/p&gt;
&lt;h2&gt;Applications and Pointers&lt;/h2&gt;
&lt;p&gt;It&#39;s been a long post, but it&#39;d be a shame not to mention some applications. There are primarily three applications that sparked my interest in the robustness of the Power Method.&lt;/p&gt;
&lt;p&gt;The first application is in &lt;strong&gt;privacy-preserving singular vector computation&lt;/strong&gt;. Here the goal is to compute the dominant singular vectors of a matrix without leaking too much information about individual entries of the matrix. We can formalize this constraint using the notion of Differential Privacy. To get a differentially private version of the Power Method we must add a significant amount of Gaussian noise at each step. To get tight error bounds it&#39;s essential to distinguish between the projections of the noise term onto the space spanned by the top singular vectors and its orthogonal complement. You can see the full analysis &lt;a href=&quot;http://arxiv.org/abs/1311.2495&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Another exciting motivation comes from the area of &lt;strong&gt;Matrix Completion&lt;/strong&gt;. Here, we only see a subsample of the matrix \({A}\) and we wish to compute its dominant singular vectors from this subsample. Alternating Minimization (as briefly mentioned above) is a popular and successful heuristic in this setting, but rather difficult to analyze. &lt;a href=&quot;http://dl.acm.org/citation.cfm?doid=2488608.2488693&quot;&gt;Jain, Netrapalli and Sanghavi&lt;/a&gt; observed that it can be seen and analyzed as an instance of the noisy power method that we discussed here. The error term in this case arises because we only see a subsample of the matrix and cannot evaluate exact inner products with \({A.}\) These observations lead to rigorous convergence bounds for Alternating Minimization. I recently &lt;a href=&quot;http://arxiv.org/abs/1312.0925&quot;&gt;posted some improvements&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Surprisingly, there&#39;s a fascinating connection between the privacy setting and the matrix completion setting. In both cases the fundamental parameter that controls the error rate is the &lt;em&gt;coherence&lt;/em&gt; of the matrix \({A}\). Arguing about the coherence of various intermediate solutions in the Power Method is a non-trivial issue that arises in both settings and is beyond the scope of this post.&lt;/p&gt;
&lt;p&gt;Finally, there are very interesting &lt;strong&gt;tensor generalizations of the Power Method &lt;/strong&gt;and robustness continues to be a key concern. See, for example, &lt;a href=&quot;http://arxiv.org/abs/1210.7559&quot;&gt;here &lt;/a&gt;and &lt;a href=&quot;http://arxiv.org/abs/1311.2972&quot;&gt;here&lt;/a&gt;. These tensor methods have a number of applications in algorithms and machine learning that are becoming increasingly relevant. While there has been some progress, I think it is fair to say that much remains to be understood in the tensor case. I may decide to work on this once my fear of tensor notation recedes to bearable levels.&lt;/p&gt;
</description>
        <pubDate>Wed, 04 Dec 2013 08:20:16 -0800</pubDate>
        <link>http://blog.mrtz.org/2013/12/04/power-method.html</link>
        <guid isPermaLink="true">http://blog.mrtz.org/2013/12/04/power-method.html</guid>
        
        <category>algorithms</category>
        
        <category>tcs</category>
        
        <category>theory</category>
        
        <category>power method</category>
        
        <category>eigenvalues</category>
        
        <category>matrix completion</category>
        
        <category>robustness</category>
        
        
      </item>
    
      <item>
        <title>The Geometric View on Sparse Recovery</title>
        <description>&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;Sparsity is of fundamental importance in much of signal processing, optimization, computer science and statistics. It was also a major theme in both workshops so far in the &lt;a href=&quot;http://simons.berkeley.edu/programs/bigdata2013&quot;&gt;Simons Big Data&lt;/a&gt; program. The second workshop on succinct representations ended a few weeks ago. Those still barely conscious after a week of technical talks were rewarded by this sunset over Berkeley. (Technically, you still would have had to climb up to the top of Centennial Dr somehow.) &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/sunset3.jpg&quot; alt=&quot;Sunset&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sparsity is a solution concept and not just a technical term. Sparse solutions to an optimization problem are often more well-behaved as they explain a phenomenon with fewer parameters. In high-dimensional settings where there are more variables than constraints, enforcing sparsity makes sense of problems that would otherwise be underdetermined.&lt;/p&gt;
&lt;p&gt;There&#39;s been so much work on sparsity it seems hard to find a place to start. Fortunately, there&#39;s a simple and elegant---yet powerful---geometric approach underlying many results in sparse recovery that I&#39;ll focus on here.&lt;/p&gt;
&lt;h2&gt;The Basic Picture&lt;/h2&gt;
&lt;p&gt;Perhaps the simplest example is solving an underdetermined system of linear equations \({Ax=b.}\) Assume \({A}\) is an \({m\times n}\) matrix and \({b=Ax_0}\) for some \({k}\)-sparse vector \({x_0}\) (i.e., \({x_0}\) has at most \({k}\) nonzero entries). We can hope to recover \({x_0}\) provided that \({A}\) has more than \({k}\) linearly independent rows. How do we do this efficiently? The problem of solving \({Ax=b}\) s.t. \({x}\) is \({k}\)-sparse is NP-hard. We will instead consider the problem&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;\(\displaystyle \min f(x)\quad\mathrm{s.t.}\,\, Ax=b \)&lt;/p&gt;
&lt;p&gt;for some convex function \({f\colon\mathbb{R}^n\rightarrow\mathbb{R}.}\) The function \({f}\) has no easy job. It needs to favor sparse solutions and yet be convex! Why should such a creature exist? Let&#39;s think about what we&#39;re asking for formally. We know that all the solutions to \({Ax=b}\) can be written as \({x_0+h}\) where \({h}\) is in the null space of \({A}\). Hence, \({x_0}\) is the solution to our problem if for all \({h}\) in the null space of \({A,}\) we have \({f(x_0+h) &amp;gt; f(x_0).}\) Consider therefore the set of all directions that would actually decrease the function value. These directions form a cone, called tangent cone, formally defined as:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;\(\displaystyle T(x_0) = \{ d \colon \exists \alpha&amp;gt;0\quad f(x_0 + \alpha d) \le f(x_0) \}. \)&lt;/p&gt;
&lt;h3&gt;The Restricted Nullspace Property&lt;/h3&gt;
&lt;p&gt;The central geometric question can then be phrased as: When is it the case that the tangent cone intersects the nullspace &lt;em&gt;only&lt;/em&gt; at the origin? Formally, we&#39;re asking:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;\(T(x_0)\cap \mathrm{null}(A)=\{ 0 \}\quad \)?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/restricted.png&quot; alt=&quot;Restricted nullspace property&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Restricted Nullspace Property is actually the name of a closely related property. Nevertheless I decided to run with this name for the purpose of this blog post.&lt;/p&gt;
&lt;p&gt;It is helpful to think of \({\mathrm{null}(A)}\) as a random subspace. This happens, for example, when \({A}\) is a random Gaussian matrix. On intuitive grounds, we should aim to make \({T(x_0)}\) as  &quot;small&quot; as possible to avoid any non-trivial intersection with the random nullspace while maintaining convexity. A natural choice is to let \({f}\) be the familiar \({\ell_1}\)-norm \({f(x)=\|x\|_1.}\) Even with these two choices though it&#39;s not obvious how to analyze the intersection of tangent cone and nullspace. We&#39;d have to argue about all points in the tangent cone. You can probably guess what&#39;s next. Instead of proving the condition directly, we&#39;ll exhibit a &quot;dual certificate&quot;. Duality is really easy here. The negative of the dual cone of \({T(x_0)}\) is the set&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;\(\displaystyle G = \{ g \colon f(y) \ge f(x_0) + \langle g,y-x\rangle \}. \)&lt;/p&gt;
&lt;p&gt;If \({f}\) were differentiable at \({x_0,}\) then \({G}\) would be the singleton set containing the gradient of \({f}\) at \({x_0.}\) Since this is not the case for the \({\ell_1}\)-norm, there is an entire cone of vectors satisfying the relation. This cone is often called the subdifferential of the \({\ell_1}\)-norm at \({x_0.}\) Its members are called &lt;a href=&quot;http://www.stanford.edu/class/ee364b/notes/subgradients_notes.pdf&quot;&gt;subgradients&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now, suppose we had a vector \({g\in G}\) such that \({g}\) was a linear combination of the rows of \({A,}\) i.e., \({g = A^t \lambda}\) for some vector \({\lambda.}\) In this case, for all \({y=x_0+h}\) with \({h}\) in the null space of \({A,}\) it must be the case that&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;\(\displaystyle f(y) \ge f(x_0) + (A^t\lambda)^t (y-x_0) = f(x_0) + \lambda^t(Ah) = f(x_0). \)&lt;/p&gt;
&lt;p&gt;In other words, such a vector \({g}\) would certify the optimality of \({x_0.}\)&lt;/p&gt;
&lt;p&gt;What we&#39;ve done so far has just been basic convex analysis and holds in great generality. Nevertheless, it makes sense to internalize this Ansatz as it is the starting point of many results in the area. (Note also my flawless command of the German language. Only Eric Price argues that I misuse the term.)&lt;/p&gt;
&lt;h3&gt;Construction of the Dual Certificate&lt;/h3&gt;
&lt;p&gt;Let&#39;s see why this approach is powerful. First, the subdifferential of the \({\ell_1}\)-norm is easy to characterize. Using some &lt;a href=&quot;http://www.stanford.edu/class/ee364b/notes/subgradients_notes.pdf&quot;&gt;basic rules&lt;/a&gt; it turns out that \({G}\) contains all the vectors \({g}\) such that:&lt;/p&gt;
&lt;p&gt;1. \({g_i}\) equals the sign of \({(x_0)_i}\) for all \({i}\) in the support of \({x_0}\) which we&#39;ll denote by \({S,}\) and&lt;/p&gt;
&lt;p&gt;2. \({g_i\in[-1,1]}\) for all \({i\in S^c.}\)&lt;/p&gt;
&lt;p&gt;So, the first condition is a set of \({|S|}\) linear equations on \({A^t\lambda.}\) Since \({A}\) has more than \({|S|}\) rows and a random Gaussian matrix is in general position, this set of equations has a solution! The second constraint bounds the \({\ell_\infty}\)-norm of \({A^t\lambda}\) outside the support of \({x_0.}\)&lt;/p&gt;
&lt;p&gt;Remarkably, the way people find a suitable \({\lambda}\) is by solving, again, a convex optimization problem! In this case, we&#39;re going to find the least squares solution to (1) and hope that magically it will satisfy (2). Actually, there isn&#39;t too much magic. A reason for this choice is this. Suppose we partition the columns of \({A}\) as \({A = [ A_1 | A_2 ]}\) according to the support of \({x_0.}\) For simplicity we assume that the support is on the first \({|S|}\) columns of \({A.}\) The least squares solution has the following appealing property. Of course, it satisfies the linear equations on \({A_1}\). Moreover, the \({\ell_\infty}\)-norm of \({A_2^t \lambda}\) is dictated by the \({\ell_2}\)-norm of \({\lambda}\). This is because \({A_1}\) and \({A_2}\) are statistically independent and \({A_2}\) is distributed as a random Gaussian matrix. This interpretation of why least squares is conceptually the right thing to look at in the Gaussian case was pointed out to me by Sushant Sachdeva.&lt;/p&gt;
&lt;p&gt;What&#39;s left is to use some probability theory to work out the parameters that we get.  For those details check out Ben Recht&#39;s talk &lt;a href=&quot;http://simons.berkeley.edu/talks/ben-recht-2013-09-05&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;The Issue of Noise&lt;/h3&gt;
&lt;p&gt;Alas, the picture isn&#39;t quite as simple as suggested so far. In almost any realistic setting we&#39;d like sparse recovery to be robust to noisy observations. This leads to several problems that are beyond the scope of this blog post. In the case of linear equations this leads to the theory of the \({\ell_1}\)-regularized least squares, usually called LASSO. Here the goal is to minimize a least squares error, while controlling the \({\ell_1}\)-norm of the solution.  Martin Wainwright&#39;s &lt;a href=&quot;http://simons.berkeley.edu/talks/martin-wainwright-2013-09-05a&quot;&gt;first talk&lt;/a&gt;  at the Big Data Boot Camp dealt with the theory of LASSO in detail.&lt;/p&gt;
&lt;h2&gt;From Vectors to Matrices: Matrix Sensing and Completion&lt;/h2&gt;
&lt;p&gt;Much of what we discussed is still true when we go from vectors to matrices. All concepts generalize by looking at the spectrum of the matrix. Sparsity in the spectrum means low rank. The \({\ell_1}\)-norm applied to the spectrum of the matrix is called &lt;strong&gt;nuclear norm&lt;/strong&gt; and plays a crucial role in the matrix world. The generalization of the above proof is most direct in the case of Matrix Sensing. Here we are given linear measurements on a matrix \({ {\cal A}(X) = B}\) where \({B={\cal A}(X_0)}\) and \({X_0}\) is a low-rank matrix. Just to be clear, \({ {\cal A}}\) is now a linear operator on matrices. One could define a random Gaussian operator by taking inner products between random Gaussian matrices and the matrix \({X.}\)&lt;/p&gt;
&lt;p&gt;A particularly well-studied linear operator is the subsampling operator. It takes \({X}\) and projects its entries to some subset \({\Omega.}\) Recovering \({X_0}\) in this case is known as Matrix Completion. It&#39;s a noticeably more difficult problem. Even if \({\Omega}\) is nice, say, uniformly random of some density, the approach can&#39;t work if \({X_0}\) hides its support in a tiny subset of the matrix. That&#39;s why we need additional assumptions. The prevailing assumption is known as incoherence. It states that the singular vectors of \({X_0}\) should be de-localized in the sense of having small inner product with the standard basis. This rules out the pathological example I just mentioned.&lt;/p&gt;
&lt;p&gt;Incoherence is often justified by saying that otherwise unique recovery were simply not possible. Why should we insist on uniquely recovering a solution? In many cases it might be interesting to find &lt;em&gt;any&lt;/em&gt; low-rank matrix consistent with the observations. Much less is known about matrix completion without an incoherence assumption in the regime where unique recovery is not possible. As far as I know, there are neither convincing hardness results (apart from NP-hardness in full generality) nor strong positive results.&lt;/p&gt;
&lt;h2&gt;Other Pointers&lt;/h2&gt;
&lt;p&gt;I&#39;ve only touched on the subject lightly. There is much more to say and several talks I haven&#39;t mentioned. Here are some:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sometimes the \({\ell_1}\)-norm is just too simple and ignores the structure of the problem in a particular application. Francis Bach &lt;a href=&quot;http://simons.berkeley.edu/talks/francis-bach-2013-09-19&quot;&gt;spoke about using submodular functions&lt;/a&gt; instead of the \({\ell_1}\)-norm. The Lovasz extension leads to a convex relaxation that makes the approach feasible.&lt;/li&gt;
&lt;li&gt;Sparsity also makes sense in settings where the signal has no coordinate structure. An example is Spectrum Estimation where the observations are combinations of few sinusoids. One could hack this application by some discretization, but a more elegant solution is possible. This was yet &lt;a href=&quot;http://simons.berkeley.edu/talks/benjamin-recht-2013-09-19&quot;&gt;another talk&lt;/a&gt; by Ben Recht.&lt;/li&gt;
&lt;li&gt;Martin Wainwright&#39;s &lt;a href=&quot;http://simons.berkeley.edu/talks/martin-wainwright-2013-09-05b&quot;&gt;second talk&lt;/a&gt; at the Boot Camp showed a very appealing general statistical framework for reasoning about regularized convex optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To follow future posts, it’s probably best to subscribe to the &lt;a href=&quot;http://mrtz.org/blog/feed/&quot;&gt;RSS feed&lt;/a&gt; or follow me on &lt;a href=&quot;http://twitter.com/mrtz&quot;&gt;Twitter&lt;/a&gt;. It&#39;s some web site that theoreticians don&#39;t use.&lt;/p&gt;
</description>
        <pubDate>Wed, 16 Oct 2013 07:58:18 -0700</pubDate>
        <link>http://blog.mrtz.org/2013/10/16/sparse-recovery.html</link>
        <guid isPermaLink="true">http://blog.mrtz.org/2013/10/16/sparse-recovery.html</guid>
        
        <category>tcs</category>
        
        <category>sparsity</category>
        
        <category>optimization</category>
        
        <category>compressed sensing</category>
        
        <category>matrix completion</category>
        
        <category>{&quot;id&quot;=&gt;84}</category>
        
        
      </item>
    
  </channel>
</rss>
